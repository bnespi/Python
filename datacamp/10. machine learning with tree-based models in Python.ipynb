{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Decision-Tree for Classification**\n",
    "___\n",
    "Classification tree\n",
    "- sequence of if-else questions about individual features\n",
    "    - **objective**: infer class labels\n",
    "- able to capture non-linear relationships between features and labels\n",
    "- do not require feature scaling (e.g., Standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train your first classification tree\n",
    "\n",
    "#In this exercise you'll work with the Wisconsin Breast Cancer Dataset\n",
    "#from the UCI machine learning repository. You'll predict whether a tumor\n",
    "#is malignant or benign based on two features: the mean radius of the\n",
    "#tumor (radius_mean) and its mean number of concave points (concave points_mean).\n",
    "\n",
    "#The dataset is already loaded in your workspace and is split into 80%\n",
    "#train and 20% test. The feature matrices are assigned to X_train and\n",
    "#X_test, while the arrays of labels are assigned to y_train and y_test\n",
    "#where class 1 corresponds to a malignant tumor and class 0 corresponds\n",
    "#to a benign tumor. To obtain reproducible results, we also defined a\n",
    "#variable called SEED which is set to 1.\n",
    "\n",
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
    "#dt = DecisionTreeClassifier(max_depth=6, random_state=SEED)\n",
    "\n",
    "# Fit dt to the training set\n",
    "#dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "#y_pred = dt.predict(X_test)\n",
    "#print(y_pred[0:5])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [0 0 0 1 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Evaluate the classification tree\n",
    "\n",
    "#Now that you've fit your first classification tree, it's time to evaluate\n",
    "#its performance on the test set. You'll do so using the accuracy metric\n",
    "#which corresponds to the fraction of correct predictions made on the test set.\n",
    "\n",
    "#The trained model dt from the previous exercise is loaded in your workspace\n",
    "#along with the test set features matrix X_test and the array of labels y_test.\n",
    "\n",
    "# Import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict test set labels\n",
    "#y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute test set accuracy\n",
    "#acc = accuracy_score(y_test, y_pred)\n",
    "#print(\"Test set accuracy: {:.2f}\".format(acc))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Test set accuracy: 0.89"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Logistic regression vs classification tree\n",
    "\n",
    "#A classification tree divides the feature space into rectangular regions.\n",
    "#In contrast, a linear model such as logistic regression produces only a\n",
    "#single linear decision boundary dividing the feature space into two\n",
    "#decision regions.\n",
    "\n",
    "#We have written a custom function called plot_labeled_decision_regions()\n",
    "#that you can use to plot the decision regions of a list containing two\n",
    "#trained classifiers. You can type help(plot_labeled_decision_regions)\n",
    "#in the IPython shell to learn more about this function.\n",
    "\n",
    "#X_train, X_test, y_train, y_test, the model dt that you've trained in\n",
    "#an earlier exercise , as well as the function plot_labeled_decision_regions()\n",
    "#are available in your workspace.\n",
    "\n",
    "# Import LogisticRegression from sklearn.linear_model\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "\n",
    "# Instantiate logreg\n",
    "logreg = LogisticRegression(random_state=1)\n",
    "\n",
    "# Fit logreg to the training set\n",
    "#logreg.fit(X_train, y_train)\n",
    "\n",
    "# Define a list called clfs containing the two classifiers logreg and dt\n",
    "#clfs = [logreg, dt]\n",
    "\n",
    "# Review the decision regions of the two classifiers\n",
    "#plot_labeled_decision_regions(X_test, y_test, clfs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![images/10.1.svg](images/10.1.svg)\n",
    "\n",
    "Notice how the decision boundary produced by logistic regression is linear while the boundaries produced by the classification tree divide the feature space into rectangular regions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Classification-Tree Learning**\n",
    "___\n",
    "-  Terms\n",
    "    1. **Decision-Tree**: data structure consisting of a hierarchy of nodes\n",
    "    2. **Node**: question or prediction\n",
    "    3. **Root**: *no* parent node, question giving rise to *two* children nodes\n",
    "    4. **Internal Node**: *one* parent node, question giving rise to *two* children nodes\n",
    "    5. **Leaf**: *one* parent node, *no* children nodes --> *prediction*\n",
    "        - in each leaf, one class label is predominant\n",
    "- Information Gain (IG)\n",
    "    - at each decision point a question regarding a split point for a feature is answered\n",
    "    - tree maximizes information by making sure each node contains information, which decides the split point\n",
    "        - IG(feature,split-point)= Impurity(parent) - (Nleft/N*Impurityleft + Nright/N*Impurityright)\n",
    "        - criteria to measure the impurity of a node:\n",
    "            - gini index\n",
    "            - entropy\n",
    "- Classification-Tree learning\n",
    "    - nodes of classification tree are grown recursively; a node exists based on the state of its predecessors\n",
    "    - At each non-leaf node data is split on:\n",
    "        - feature and split-point to maximize IG(node)\n",
    "        - if IG(node)=0, declare the node a leaf (or if the tree is constrained to a certain amount of levels)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Using entropy as a criterion\n",
    "\n",
    "#In this exercise, you'll train a classification tree on the Wisconsin\n",
    "#Breast Cancer dataset using entropy as an information criterion. You'll\n",
    "#do so using all the 30 features in the dataset, which is split into 80%\n",
    "#train and 20% test.\n",
    "\n",
    "#X_train as well as the array of labels y_train are available in your workspace.\n",
    "\n",
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
    "dt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)\n",
    "\n",
    "# Fit dt_entropy to the training set\n",
    "#dt_entropy.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Entropy vs Gini index\n",
    "#In this exercise you'll compare the test set accuracy of dt_entropy to\n",
    "#the accuracy of another tree named dt_gini. The tree dt_gini was trained\n",
    "#on the same dataset using the same parameters except for the information\n",
    "#criterion which was set to the gini index using the keyword 'gini'.\n",
    "\n",
    "#X_test, y_test, dt_entropy, as well as accuracy_gini which corresponds to\n",
    "#the test set accuracy achieved by dt_gini are available in your workspace.\n",
    "\n",
    "#Notice how the two models achieve exactly the same accuracy. Most of the\n",
    "#time, the gini index and entropy lead to the same results. The gini index\n",
    "#is slightly faster to compute and is the default criterion used in the\n",
    "#DecisionTreeClassifier model of scikit-learn.\n",
    "\n",
    "# Import accuracy_score from sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Use dt_entropy to predict test set labels\n",
    "#y_pred = dt_entropy.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_entropy\n",
    "#accuracy_entropy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy_entropy\n",
    "#print('Accuracy achieved by using entropy: ', accuracy_entropy)\n",
    "\n",
    "# Print accuracy_gini\n",
    "#print('Accuracy achieved by using the gini index: ', accuracy_gini)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Accuracy achieved by using entropy:  0.929824561404\n",
    "#    Accuracy achieved by using the gini index:  0.929824561404"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Decision tree for regression**\n",
    "___\n",
    "- good for applications when relationships between features are non-linear\n",
    "    - min_samples_leaf ==> how much of training data each leaf must contain at a minimum.\n",
    "- Information criterion for regression-tree\n",
    "- I(node) = MSE(node) = 1/Nnode * sum (y^i-ynodemean)^2\n",
    "    - ynodemean = 1/Nnode * sum (y^i)\n",
    "- Prediction\n",
    "    - ypredmean(leaf) = 1/Nleaf * sum (y^i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train your first regression tree\n",
    "\n",
    "#In this exercise, you'll train a regression tree to predict the mpg\n",
    "#(miles per gallon) consumption of cars in the auto-mpg dataset using\n",
    "#all the six available features.\n",
    "\n",
    "#The dataset is processed for you and is split to 80% train and 20%\n",
    "#test. The features matrix X_train and the array y_train are available\n",
    "#in your workspace.\n",
    "\n",
    "# Import DecisionTreeRegressor from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeRegressor(max_depth=8,\n",
    "             min_samples_leaf=0.13,\n",
    "            random_state=3)\n",
    "\n",
    "# Fit dt to the training set\n",
    "#dt.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute y_pred\n",
    "#y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute mse_dt\n",
    "#mse_dt = MSE(y_pred, y_test)\n",
    "\n",
    "# Compute rmse_dt\n",
    "#rmse_dt = mse_dt**(1/2)\n",
    "\n",
    "# Print rmse_dt\n",
    "#print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Test set RMSE of dt: 4.37"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Linear regression vs regression tree\n",
    "\n",
    "#In this exercise, you'll compare the test set RMSE of dt to that\n",
    "#achieved by a linear regression model. We have already instantiated a\n",
    "#linear regression model lr and trained it on the same dataset as dt.\n",
    "\n",
    "#The features matrix X_test, the array of labels y_test, the trained\n",
    "#linear regression model lr, mean_squared_error function which was\n",
    "#imported under the alias MSE and rmse_dt from the previous exercise\n",
    "#are available in your workspace.\n",
    "\n",
    "# Predict test set labels\n",
    "#y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Compute mse_lr\n",
    "#mse_lr = MSE(y_pred_lr, y_test)\n",
    "\n",
    "# Compute rmse_lr\n",
    "#rmse_lr = mse_lr**(1/2)\n",
    "\n",
    "# Print rmse_lr\n",
    "#print('Linear Regression test set RMSE: {:.2f}'.format(rmse_lr))\n",
    "\n",
    "# Print rmse_dt\n",
    "#print('Regression Tree test set RMSE: {:.2f}'.format(rmse_dt))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Linear Regression test set RMSE: 5.10\n",
    "#    Regression Tree test set RMSE: 4.37"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Generalization Error**\n",
    "___\n",
    "- Supervised learning - Under the hood\n",
    "    - y = f(x) where f is unknown\n",
    "    - **goal**: find a model that best approximates f\n",
    "        - discard noise as much as possible\n",
    "    - **end goal**: model should achieve a low predictive error on unseen datasets.\n",
    "- Difficulties in approximating f(x)\n",
    "    - **overfitting**: when model fits the noise in the training set.\n",
    "        - predictive power on unseen datasets is low\n",
    "    - **underfitting**: when model is not flexible enough to approximate f(x)\n",
    "        - training set error is roughly equal to test set error\n",
    "        - \"like teaching calculus to a 3-year-old\"\n",
    "- Generalization error\n",
    "    - does model generalize well on unseen data?\n",
    "    - = bias^2 + variance + irreducible error\n",
    "        - **bias**: error term that tells you, on average, how much model is different from f(x)\n",
    "            - high bias models lead to underfitting\n",
    "        - **variance**: tells you how much model is inconsistent over different training sets.\n",
    "            - high variance models lead to overfitting\n",
    "    - model complexity (e.g., maximum tree depth) sets the flexibility of model\n",
    "        - best model complexity = lowest generalization error\n",
    "        - not enough depth = underfitting\n",
    "        - too much depth = overfitting\n",
    "    - bias-variance tradeoff\n",
    "        - as one increases, the other decreases\n",
    "        - irreducible error is constant\n",
    "        - where is the sweet spot?\n",
    "        - analogous to validity-reliability\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Diagnose bias and variance problems**\n",
    "___\n",
    "- Estimating the generalization error\n",
    "    - cannot be done directly because:\n",
    "        - f is unknown\n",
    "        - usually have only one dataset\n",
    "        - noise is unpredictable\n",
    "    - Solution:\n",
    "        - split data into training and test set\n",
    "        - fit model to training set\n",
    "        - evaluate error of model on test set\n",
    "        - generalization error of model is roughly equivalent to test set error of model\n",
    "        - evaluating model on training set provides a biased estimate as model has already seen all of these data points\n",
    "        - Solution: cross-validation\n",
    "- K-Fold Cross-Validation (CV)\n",
    "    - training set is randomly separated into K number of partitions\n",
    "    - error is calculated for each fold/partition separately\n",
    "        - one fold is evaluated on test set while other folds are trained\n",
    "        - this is done k times.\n",
    "    - CVerror = mean(k errors)\n",
    "- if model suffers from **high variance**\n",
    "    - CV error of model > training set error of model\n",
    "    - model is overfitting the training set\n",
    "        - decrease model complexity (e.g., decrease max depth, increase min samples per leaf)\n",
    "        - gather more data\n",
    "- if model suffers from **high bias**\n",
    "    - CV error of model is roughly equivalent to training set error of model which is > desired error\n",
    "    - model is underfitting the training set\n",
    "        - increase model complexity (e.g., increase max depth, decrease min samples per leaf)\n",
    "        - gather more relevant features\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Instantiate the model\n",
    "\n",
    "#In the following set of exercises, you'll diagnose the bias and variance\n",
    "#problems of a regression tree. The regression tree you'll define in this\n",
    "#exercise will be used to predict the mpg consumption of cars from the auto\n",
    "#dataset using all available features.\n",
    "\n",
    "#We have already processed the data and loaded the features matrix X and\n",
    "#the array y in your workspace. In addition, the DecisionTreeRegressor\n",
    "#class was imported from sklearn.tree.\n",
    "\n",
    "# Import train_test_split from sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set SEED for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split the data into 70% train and 30% test\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=SEED)\n",
    "\n",
    "# Instantiate a DecisionTreeRegressor dt\n",
    "#dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.26, random_state=SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Evaluate the 10-fold CV error\n",
    "\n",
    "#In this exercise, you'll evaluate the 10-fold CV Root Mean Squared Error\n",
    "#(RMSE) achieved by the regression tree dt that you instantiated in the\n",
    "#previous exercise.\n",
    "\n",
    "#In addition to dt, the training data including X_train and y_train are\n",
    "#available in your workspace. We also imported cross_val_score from\n",
    "#sklearn.model_selection.\n",
    "\n",
    "#Note that since cross_val_score has only the option of evaluating the\n",
    "#negative MSEs, its output should be multiplied by negative one to obtain\n",
    "#the MSEs. The CV RMSE can then be obtained by computing the square root\n",
    "#of the average MSE.\n",
    "\n",
    "#A very good practice is to keep the test set untouched until you are\n",
    "#confident about your model's performance. CV is a great technique to\n",
    "#get an estimate of a model's performance without affecting the test set.\n",
    "\n",
    "# Compute the array containing the 10-folds CV MSEs\n",
    "#MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10,\n",
    "#                                  scoring='neg_mean_squared_error',\n",
    "#                                  n_jobs=-1)\n",
    "\n",
    "# Compute the 10-folds CV RMSE\n",
    "#RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
    "\n",
    "# Print RMSE_CV\n",
    "#print('CV RMSE: {:.2f}'.format(RMSE_CV))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    CV RMSE: 5.14"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Evaluate the training error\n",
    "\n",
    "#You'll now evaluate the training set RMSE achieved by the regression\n",
    "#tree dt that you instantiated in a previous exercise.\n",
    "\n",
    "#In addition to dt, X_train and y_train are available in your workspace.\n",
    "\n",
    "#Note that in scikit-learn, the MSE of a model can be computed as follows:\n",
    "\n",
    "#MSE_model = mean_squared_error(y_true, y_predicted)\n",
    "#where we use the function mean_squared_error from the metrics module\n",
    "#and pass it the true labels y_true as a first argument, and the predicted\n",
    "#labels from the model y_predicted as a second argument.\n",
    "\n",
    "#Notice how the training error is roughly equal to the 10-folds CV error\n",
    "#you obtained in the previous exercise, i.e. underfitting\n",
    "\n",
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Fit dt to the training set\n",
    "#dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the training set\n",
    "#y_pred_train = dt.predict(X_train)\n",
    "\n",
    "# Evaluate the training set RMSE of dt\n",
    "#RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
    "\n",
    "# Print RMSE_train\n",
    "#print('Train RMSE: {:.2f}'.format(RMSE_train))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Train RMSE: 5.15"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ensemble Learning**\n",
    "___\n",
    "Classification and Regression Trees\n",
    "- Advantages of CARTs\n",
    "    - ability to describe non-linear dependencies\n",
    "    - no need to standardize or normalize features\n",
    "- Limitations of CARTs\n",
    "    - can only produce orthogonal decision boundaries in classification\n",
    "    - sensitive to small variations in the training set\n",
    "    - unconstrained CARTs may have high variance and overfit the training set\n",
    "        - **solution**: ensemble learning\n",
    "- Ensemble learning\n",
    "    - train different models on same dataset\n",
    "    - each model makes predictions\n",
    "    - a meta-model aggregates predictions of individual models\n",
    "    - final prediction is more robust and less prone to errors\n",
    "    - Voting Classifier\n",
    "        - i.e. best 2 out of 3 or majority rules.\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Define the ensemble\n",
    "\n",
    "#In the following set of exercises, you'll work with the Indian Liver\n",
    "#Patient Dataset from the UCI Machine learning repository.\n",
    "\n",
    "#In this exercise, you'll instantiate three classifiers to predict\n",
    "#whether a patient suffers from a liver disease using all the features\n",
    "#present in the dataset.\n",
    "\n",
    "#The classes LogisticRegression, DecisionTreeClassifier, and\n",
    "#KNeighborsClassifier under the alias KNN are available in your workspace.\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED=1\n",
    "\n",
    "# Instantiate lr\n",
    "#lr = LogisticRegression(random_state=SEED)\n",
    "\n",
    "# Instantiate knn\n",
    "#knn = KNN(n_neighbors=27)\n",
    "\n",
    "# Instantiate dt\n",
    "#dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n",
    "\n",
    "# Define the list classifiers\n",
    "#classifiers = [('Logistic Regression', lr), ('K Nearest Neighbors', knn), ('Classification Tree', dt)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Evaluate individual classifiers\n",
    "\n",
    "#In this exercise you'll evaluate the performance of the models in the\n",
    "#list classifiers that we defined in the previous exercise. You'll do\n",
    "#so by fitting each classifier on the training set and evaluating its\n",
    "#test set accuracy.\n",
    "\n",
    "#The dataset is already loaded and preprocessed for you (numerical\n",
    "#features are standardized) and it is split into 70% train and 30% test.\n",
    "#The features matrices X_train and X_test, as well as the arrays of\n",
    "#labels y_train and y_test are available in your workspace. In addition,\n",
    "#we have loaded the list classifiers from the previous exercise, as well\n",
    "#as the function accuracy_score() from sklearn.metrics.\n",
    "\n",
    "# Iterate over the pre-defined list of classifiers\n",
    "#for clf_name, clf in classifiers:\n",
    "\n",
    "    # Fit clf to the training set\n",
    "#    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict y_pred\n",
    "#    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "#    accuracy = accuracy_score(y_pred, y_test)\n",
    "\n",
    "    # Evaluate clf's accuracy on the test set\n",
    "#    print('{:s} : {:.3f}'.format(clf_name, accuracy))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Logistic Regression : 0.747\n",
    "#    K Nearest Neighbours : 0.724\n",
    "#    Classification Tree : 0.730"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Better performance with a Voting Classifier\n",
    "\n",
    "#Finally, you'll evaluate the performance of a voting classifier that\n",
    "#takes the outputs of the models defined in the list classifiers and\n",
    "#assigns labels by majority voting.\n",
    "\n",
    "#X_train, X_test,y_train, y_test, the list classifiers defined in a\n",
    "#previous exercise, as well as the function accuracy_score from\n",
    "#sklearn.metrics are available in your workspace.\n",
    "\n",
    "# Import VotingClassifier from sklearn.ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Instantiate a VotingClassifier vc\n",
    "#vc = VotingClassifier(estimators=classifiers)\n",
    "\n",
    "# Fit vc to the training set\n",
    "#vc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the test set predictions\n",
    "#y_pred = vc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "#accuracy = accuracy_score(y_pred, y_test)\n",
    "#print('Voting Classifier: {:.3f}'.format(accuracy))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#   Voting Classifier: 0.753"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Bagging**\n",
    "___\n",
    "Bootstrap aggregation\n",
    "- **Voting Classifier**\n",
    "    - same training set\n",
    "    - != algorithms\n",
    "- **Bagging**\n",
    "    - != subsets of the training set\n",
    "    - one algorithm\n",
    "    - reduces variance of individual models in the ensemble\n",
    "    - bootstrapping refers to the activity of sampling with replacement\n",
    "    - **Classification**\n",
    "        - aggregates predictions by majority voting\n",
    "        - BaggingClassifier in sklearn\n",
    "    - *Regression**\n",
    "        - aggregates predictions through averaging\n",
    "        - BaggingRegressor in sklearn\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#Define the bagging classifier\n",
    "\n",
    "#In the following exercises you'll work with the Indian Liver Patient\n",
    "#dataset from the UCI machine learning repository. Your task is to predict\n",
    "#whether a patient suffers from a liver disease using 10 features including\n",
    "#Albumin, age and gender. You'll do so using a Bagging Classifier.\n",
    "\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Evaluate Bagging performance\n",
    "\n",
    "#Now that you instantiated the bagging classifier, it's time to train\n",
    "#it and evaluate its test set accuracy.\n",
    "\n",
    "#The Indian Liver Patient dataset is processed for you and split into\n",
    "#80% train and 20% test. The feature matrices X_train and X_test, as\n",
    "#well as the arrays of labels y_train and y_test are available in your\n",
    "#workspace. In addition, we have also loaded the bagging classifier bc\n",
    "#that you instantiated in the previous exercise and the function\n",
    "#accuracy_score() from sklearn.metrics.\n",
    "\n",
    "# Fit bc to the training set\n",
    "#bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "#y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate acc_test\n",
    "#acc_test = accuracy_score(y_pred, y_test)\n",
    "#print('Test set accuracy of bc: {:.2f}'.format(acc_test))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Test set accuracy of bc: 0.71\n",
    "#################################################\n",
    "#A single tree dt would have achieved an accuracy of 63% which is\n",
    "#8% lower than bc's accuracy!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Out of Bag Evaluation**\n",
    "___\n",
    "- instance combinations that are not sampled using bootstrap aggregation during training\n",
    "- OOB score is average of all OOB instances\n",
    "- can be used as a tool for evaluating model performance without using cross-validation\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#Prepare the ground\n",
    "\n",
    "#In the following exercises, you'll compare the OOB accuracy to the test\n",
    "#set accuracy of a bagging classifier trained on the Indian Liver Patient\n",
    "#dataset.\n",
    "\n",
    "#In sklearn, you can evaluate the OOB accuracy of an ensemble classifier\n",
    "#by setting the parameter oob_score to True during instantiation. After\n",
    "#training the classifier, the OOB accuracy can be obtained by accessing\n",
    "#the .oob_score_ attribute from the corresponding instance.\n",
    "\n",
    "#In your environment, we have made available the class DecisionTreeClassifier\n",
    "#from sklearn.tree.\n",
    "\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=8, random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt,\n",
    "            n_estimators=50,\n",
    "            oob_score=True,\n",
    "            random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#OOB Score vs Test Set Score\n",
    "\n",
    "#Now that you instantiated bc, you will fit it to the training set and\n",
    "#evaluate its test set and OOB accuracies.\n",
    "\n",
    "#The dataset is processed for you and split into 80% train and 20% test.\n",
    "#The feature matrices X_train and X_test, as well as the arrays of labels\n",
    "#y_train and y_test are available in your workspace. In addition, we have\n",
    "#also loaded the classifier bc instantiated in the previous exercise and\n",
    "#the function accuracy_score() from sklearn.metrics.\n",
    "\n",
    "# Fit bc to the training set\n",
    "#bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "#y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "#acc_test = accuracy_score(y_pred, y_test)\n",
    "\n",
    "# Evaluate OOB accuracy\n",
    "#acc_oob = bc.oob_score_\n",
    "\n",
    "# Print acc_test and acc_oob\n",
    "#print('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Test set accuracy: 0.698, OOB accuracy: 0.704"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Random Forests (RF)**\n",
    "___\n",
    "- ensemble method that uses a decision tree as its base estimator\n",
    "- each estimator is trained on a different bootstrap sample having the same size as the training set\n",
    "- RF introduces further randomization in the training of individual trees\n",
    "    - *d* features are sampled at each split/node without replacement\n",
    "    - *d* < total number of features\n",
    "    - each tree is trained on a different bootstrap sample from the training set\n",
    "- achieves lower variance than individual trees\n",
    "- **Classification**\n",
    "    - Aggregates predictions by majority voting\n",
    "    - RandomForestClassifier in sklearn\n",
    "- **Regression**\n",
    "    - Aggregates predictions through averaging\n",
    "    - RandomForest Regressor in sklearn\n",
    "- Feature importance\n",
    "    - Tree-based methods enable measuring of importance of each feature in prediction\n",
    "    - in sklearn, this is how much the tree nodes use a particular feature (weighted average) to reduce impurity\n",
    "    - accessed using attribute feature_importance_\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Train an RF regressor\n",
    "#In the following exercises you'll predict bike rental demand in the Capital\n",
    "#Bikeshare program in Washington, D.C using historical weather data from the\n",
    "#Bike Sharing Demand dataset available through Kaggle. For this purpose, you\n",
    "#will be using the random forests algorithm. As a first step, you'll define\n",
    "#a random forests regressor and fit it to the training set.\n",
    "\n",
    "#The dataset is processed for you and split into 80% train and 20% test.\n",
    "#The features matrix X_train and the array y_train are available in your\n",
    "#workspace.\n",
    "\n",
    "# Import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate rf\n",
    "rf = RandomForestRegressor(n_estimators=25,\n",
    "            random_state=2)\n",
    "\n",
    "# Fit rf to the training set\n",
    "#rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Evaluate the RF regressor\n",
    "\n",
    "#You'll now evaluate the test set RMSE of the random forests regressor\n",
    "#rf that you trained in the previous exercise.\n",
    "\n",
    "#The dataset is processed for you and split into 80% train and 20% test.\n",
    "#The features matrix X_test, as well as the array y_test are available in\n",
    "#your workspace. In addition, we have also loaded the model rf that you\n",
    "#trained in the previous exercise.\n",
    "\n",
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Predict the test set labels\n",
    "#y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "#rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print rmse_test\n",
    "#print('Test set RMSE of rf: {:.2f}'.format(rmse_test))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Test set RMSE of rf: 51.97\n",
    "#################################################\n",
    "#You can try training a single CART on the same dataset. The test set\n",
    "#RMSE achieved by rf is significantly smaller than that achieved by a\n",
    "#single CART!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visualizing features importances\n",
    "\n",
    "#In this exercise, you'll determine which features were the most predictive\n",
    "#according to the random forests regressor rf that you trained in a previous\n",
    "#exercise.\n",
    "\n",
    "#For this purpose, you'll draw a horizontal barplot of the feature importance\n",
    "#as assessed by rf. Fortunately, this can be done easily thanks to plotting\n",
    "#capabilities of pandas.\n",
    "\n",
    "#We have created a pandas.Series object called importances containing the\n",
    "#feature names as index and their importances as values. In addition,\n",
    "#matplotlib.pyplot is available as plt and pandas as pd.\n",
    "\n",
    "# Create a pd.Series of features importances\n",
    "#importances = pd.Series(data=rf.feature_importances_,\n",
    "#                       index= X_train.columns)\n",
    "\n",
    "# Sort importances\n",
    "#importances_sorted = importances.sort_values()\n",
    "\n",
    "# Draw a horizontal barplot of importances_sorted\n",
    "#importances_sorted.plot(kind='barh', color='lightgreen')\n",
    "#plt.title('Features Importances')\n",
    "#plt.show()\n",
    "\n",
    "#################################################\n",
    "#In [1]: X_train.columns\n",
    "#Out[1]:\n",
    "#Index(['hr', 'holiday', 'workingday', 'temp', 'hum', 'windspeed', 'instant',\n",
    "#       'mnth', 'yr', 'Clear to partly cloudy', 'Light Precipitation', 'Misty'],\n",
    "#      dtype='object')\n",
    "\n",
    "#In [2]: rf.feature_importances_\n",
    "#Out[2]:\n",
    "#array([  7.67785168e-01,   3.82731944e-03,   1.45434673e-01,\n",
    "#         1.88155187e-02,   2.59739711e-02,   7.00403334e-03,\n",
    "#         2.19497935e-02,   5.69578113e-04,   0.00000000e+00,\n",
    "#         2.23504749e-03,   4.95581152e-03,   1.44908498e-03])\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![images/10.2.svg](images/10.2.svg)\n",
    "\n",
    "Apparently, hr and workingday are the most important features according to rf. The importances of these two features add up to more than 90%!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Adaboost**\n",
    "___\n",
    "- **Boosting**\n",
    "    - ensemble method combinging several weak learners to form a strong learner\n",
    "    - **weak learner** - model doing slightly better than random guessing\n",
    "    - example: decision stump (CART with maximum depth of 1)\n",
    "    - ensemble of predictors are trained sequentially where each predictor tries to correct its predecessor\n",
    "    - Most popular methods\n",
    "        - AdaBoost\n",
    "        - Gradient Boosting\n",
    "- **Adaboost**\n",
    "    - stands for adaptive boosting\n",
    "    - each predictor pays more attention to the instances wrongly predicted by its predecessor\n",
    "    - achieved by changing the weights of training instances\n",
    "    - each predictor is assigned a coefficient alpha\n",
    "        - alpha depends on the predictor's training error\n",
    "    - learning rate eta\n",
    "        - between 0 and 1\n",
    "        - used to shrink coefficient alpha of a trained predictor\n",
    "    - Classification\n",
    "        - Weighted majority voting\n",
    "        - in sklearn: AdaBoostClassifier\n",
    "    - Regression\n",
    "        - Weighted average\n",
    "        - in sklearn: AdaBoostRegressor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#Define the AdaBoost classifier\n",
    "\n",
    "#In the following exercises you'll revisit the Indian Liver Patient\n",
    "#dataset which was introduced in a previous chapter. Your task is to\n",
    "#predict whether a patient suffers from a liver disease using 10 features\n",
    "#including Albumin, age and gender. However, this time, you'll be training\n",
    "#an AdaBoost ensemble to perform the classification task. In addition,\n",
    "#given that this dataset is imbalanced, you'll be using the ROC AUC score\n",
    "#as a metric instead of accuracy.\n",
    "\n",
    "#As a first step, you'll start by instantiating an AdaBoost classifier.\n",
    "\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "\n",
    "# Instantiate ada\n",
    "ada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train the AdaBoost classifier\n",
    "\n",
    "#Now that you've instantiated the AdaBoost classifier ada, it's time to\n",
    "#train it. You will also predict the probabilities of obtaining the\n",
    "#positive class in the test set. This can be done as follows:\n",
    "\n",
    "#Once the classifier ada is trained, call the .predict_proba() method\n",
    "#by passing X_test as a parameter and extract these probabilities by\n",
    "#slicing all the values in the second column as follows:\n",
    "\n",
    "#ada.predict_proba(X_test)[:,1]\n",
    "#The Indian Liver dataset is processed for you and split into 80% train\n",
    "#and 20% test. Feature matrices X_train and X_test, as well as the arrays\n",
    "#of labels y_train and y_test are available in your workspace. In addition,\n",
    "#we have also loaded the instantiated model ada from the previous exercise.\n",
    "\n",
    "# Fit ada to the training set\n",
    "#ada.fit(X_train, y_train)\n",
    "\n",
    "# Compute the probabilities of obtaining the positive class\n",
    "#y_pred_proba = ada.predict_proba(X_test)[:,1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Evaluate the AdaBoost classifier\n",
    "\n",
    "#Now that you're done training ada and predicting the probabilities of\n",
    "#obtaining the positive class in the test set, it's time to evaluate\n",
    "#ada's ROC AUC score. Recall that the ROC AUC score of a binary classifier\n",
    "#can be determined using the roc_auc_score() function from sklearn.metrics.\n",
    "\n",
    "#The arrays y_test and y_pred_proba that you computed in the previous exercise\n",
    "#are available in your workspace.\n",
    "\n",
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "#ada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print roc_auc_score\n",
    "#print('ROC AUC score: {:.2f}'.format(ada_roc_auc))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    ROC AUC score: 0.71"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Gradient Boosting**\n",
    "___\n",
    "- sequential correction of predecessor's errors\n",
    "- does not tweak weights of training instances\n",
    "- each predictor is trained using its predecessor's residual errors as labels\n",
    "- Gradient Boosted Trees: a CART is used as a base learner\n",
    "- **shrinkage**\n",
    "    - prediction of each tree in the ensemble is shrunk after it is multiplied by the learning rate eta\n",
    "- **prediction**\n",
    "    - Regression\n",
    "        - ypred = y1 + eta\\*res1 +...+ eta\\*resn\n",
    "        - sklearn: GradientBoostingRegressor\n",
    "    - Classification\n",
    "        - sklearn: GradientBoostingClassifier\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#Define the GB regressor\n",
    "\n",
    "#You'll now revisit the Bike Sharing Demand dataset that was introduced\n",
    "#in the previous chapter. Recall that your task is to predict the bike\n",
    "#rental demand using historical weather data from the Capital Bikeshare\n",
    "#program in Washington, D.C.. For this purpose, you'll be using a gradient\n",
    "#boosting regressor.\n",
    "\n",
    "#As a first step, you'll start by instantiating a gradient boosting regressor\n",
    "#which you will train in the next exercise.\n",
    "\n",
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate gb\n",
    "gb = GradientBoostingRegressor(max_depth=4,\n",
    "            n_estimators=200,\n",
    "            random_state=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train the GB regressor\n",
    "\n",
    "#You'll now train the gradient boosting regressor gb that you instantiated\n",
    "#in the previous exercise and predict test set labels.\n",
    "\n",
    "#The dataset is split into 80% train and 20% test. Feature matrices X_train\n",
    "#and X_test, as well as the arrays y_train and y_test are available in your\n",
    "#workspace. In addition, we have also loaded the model instance gb that you\n",
    "#defined in the previous exercise.\n",
    "\n",
    "# Fit gb to the training set\n",
    "#gb.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "#y_pred = gb.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Evaluate the GB regressor\n",
    "\n",
    "#Now that the test set predictions are available, you can use them to\n",
    "#evaluate the test set Root Mean Squared Error (RMSE) of gb.\n",
    "\n",
    "#y_test and predictions y_pred are available in your workspace.\n",
    "\n",
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute MSE\n",
    "#mse_test = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute RMSE\n",
    "#rmse_test = mse_test**(1/2)\n",
    "\n",
    "# Print RMSE\n",
    "#print('Test set RMSE of gb: {:.3f}'.format(rmse_test))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Test set RMSE of gb: 52.065\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Stochastic Gradient Boosting (SGB)**\n",
    "___\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}