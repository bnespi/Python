{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**scikit-learn refresher**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#KNN classification\n",
    "\n",
    "#In this exercise you'll explore a subset of the Large Movie Review Dataset.\n",
    "#'The variables X_train, X_test, y_train, and y_test are already loaded\n",
    "#into the environment. The X variables contain features based on the words\n",
    "#in the movie reviews, and the y variables contain labels for whether the\n",
    "#review sentiment is positive (+1) or negative (-1).\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create and fit the model\n",
    "knn = KNeighborsClassifier()\n",
    "#knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test features, print the results\n",
    "#pred = knn.predict(X_test)[0]\n",
    "#print(\"Prediction for test example 0:\", pred)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Prediction for test example 0: 1.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Comparing models\n",
    "#create two instances of KNeighborsClassifier with n_neighbors=1, 5\n",
    "#fit training data to both instances\n",
    "\n",
    "#test for accuracy\n",
    "#knn1.score(X_test, y_test)\n",
    "#knn5.score(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Review:\n",
    "- **Underfitting**: model is too simple, low training accuracy\n",
    "- **Overfitting**: model is too complex, low test accuracy\n",
    "\n",
    "an example of **overfitting**:\n",
    "- Training accuracy 95%, testing accuracy 50%.\n",
    "\n",
    "*overfitting refers to doing better on the training set than the test set.*\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Applying logistic regression and Support Vector Machines/Classifier (SVM/SVC)**\n",
    "___\n",
    "- SVC - non-linear SVM by default"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p1n3d\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9688888888888889\n",
      "0.9955456570155902\n",
      "0.9888888888888889\n"
     ]
    }
   ],
   "source": [
    "#Running LogisticRegression and SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
    "\n",
    "# Apply logistic regression and print scores\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))\n",
    "\n",
    "# Apply SVM and print scores\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "print(svm.score(X_train, y_train))\n",
    "print(svm.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: LOVED IT! This movie was amazing. Top 10 this year.\n",
      "Review: Total junk! I'll never watch a film by that director again, no matter how good the reviews.\n"
     ]
    }
   ],
   "source": [
    "#Sentiment analysis for movie reviews\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate logistic regression and train\n",
    "lr = LogisticRegression()\n",
    "#lr.fit(X, y)\n",
    "\n",
    "# Predict sentiment for a glowing review\n",
    "review1 = \"LOVED IT! This movie was amazing. Top 10 this year.\"\n",
    "#review1_features = get_features(review1)\n",
    "print(\"Review:\", review1)\n",
    "#print(\"Probability of positive review:\", lr.predict_proba(review1_features)[0,1])\n",
    "\n",
    "# Predict sentiment for a poor review\n",
    "review2 = \"Total junk! I'll never watch a film by that director again, no matter how good the reviews.\"\n",
    "#review2_features = get_features(review2)\n",
    "print(\"Review:\", review2)\n",
    "#print(\"Probability of positive review:\", lr.predict_proba(review2_features)[0,1])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Review: LOVED IT! This movie was amazing. Top 10 this year.\n",
    "#    Probability of positive review: 0.8079007873616059\n",
    "#    Review: Total junk! I'll never watch a film by that director again, no matter how good the reviews.\n",
    "#    Probability of positive review: 0.5855117402793947"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Linear classifiers**\n",
    "___\n",
    "Definitions\n",
    "- **classification**: learning to predict categories\n",
    "- **decision boundary**: the surface separating different predicted classes\n",
    "- **linear classifier**: a classifier that learns linear decision boundaries\n",
    "    - e.g., logistic regression, linear SVM\n",
    "- **linearly separable**: a data set can be perfectly explained by a linear classifier\n",
    "___\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p1n3d\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\p1n3d\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Visualizing decision boundaries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "wine = datasets.load_wine()\n",
    "X=wine.data\n",
    "y=wine.target\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = [LogisticRegression(), LinearSVC(),\n",
    "               SVC(), KNeighborsClassifier()]\n",
    "\n",
    "# Fit the classifiers\n",
    "for c in classifiers:\n",
    "    c.fit(X, y)\n",
    "\n",
    "# Plot the classifiers - plot is a series of previously defined functions\n",
    "#plot_4_classifiers(X, y, classifiers)\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![images/9.1.svg](images/9.1.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Linear classifiers: the coefficients**\n",
    "___\n",
    "- Dot products\n",
    "    - multiply each element in arrays\n",
    "    - sum elements in remaining array\n",
    "    - x@y\n",
    "- Linear classifier prediction\n",
    "    - raw model output = coefficients[.coef_] @ features + intercept[.intercept_]\n",
    "    - if positive predict one class; if negative predict the other class\n",
    "- This is the same for logistic regression and linear SVM\n",
    "    - i.e. 'fit' is different but 'predict' is the same\n",
    "- **intercept** changes boundary location but not orientation\n",
    "- **coefficients** change orientation of boundary line\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Changing the model coefficients\n",
    "\n",
    "#When you call fit with scikit-learn, the logistic regression coefficients\n",
    "#are automatically learned from your dataset. In this exercise you will\n",
    "#explore how the decision boundary is represented by the coefficients. To\n",
    "#do so, you will change the coefficients manually (instead of with fit),\n",
    "#and visualize the resulting classifiers.\n",
    "\n",
    "#A 2D dataset is already loaded into the environment as X and y,\n",
    "#along with a linear classifier object model\n",
    "\n",
    "# Set the coefficients\n",
    "#model.coef_ = np.array([[0,1]])\n",
    "#model.intercept_ = np.array([0])\n",
    "\n",
    "# Plot the data and decision boundary using preset function\n",
    "#plot_classifier(X,y,model)\n",
    "\n",
    "# Print the number of errors\n",
    "#num_err = np.sum(y != model.predict(X))\n",
    "#print(\"Number of errors:\", num_err)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Number of errors: 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![images/9.2.svg](images/9.2.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**What is a loss function?**\n",
    "___\n",
    "- Least squares: the squared loss (linear regression)\n",
    "    - minimizes square of the error made on training set\n",
    "    - $$\\sum_{i=1}^{n}(\\text{true ith target value - predicted ith target value})^{2}$$\n",
    "- **loss function**\n",
    "    - penalty score that tells us how well/poorly model is doing on training data\n",
    "- **fit function**\n",
    "    - minimizes loss\n",
    "- *squared loss/error is not appropriate for classification problems*\n",
    "    - 0-1 loss\n",
    "    - 0 for correct prediction, 1 for incorrect prediction\n",
    "        - a natural loss for classification problem is the number of errors\n",
    "- minimizing loss using Python\n",
    "    - from **scipy.optimize** import **minimize**\n",
    "    - inputs are values of model coefficents\n",
    "    - what values of the model coefficients make my squared error as small as possible?\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Minimizing a loss function\n",
    "\n",
    "#In this exercise you'll implement linear regression \"from scratch\" using\n",
    "#scipy.optimize.minimize.\n",
    "\n",
    "#We'll train a model on the Boston housing price data set, which is\n",
    "#already loaded into the variables X and y. For simplicity, we won't\n",
    "#include an intercept in our regression model.\n",
    "\n",
    "# The squared error, summed over training examples\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        # Get the true and predicted target values for example 'i'\n",
    "        y_i_true = y[i]\n",
    "        y_i_pred = w@X[i]\n",
    "        s = s + (y_i_true - y_i_pred)**2\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "#w_fit = minimize(my_loss, X[0]).x\n",
    "#print(w_fit)\n",
    "\n",
    "# Compare with scikit-learn's LinearRegression coefficients\n",
    "#lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
    "#print(lr.coef_)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [-9.16299112e-02  4.86754828e-02 -3.77698794e-03  2.85635998e+00\n",
    "#     -2.88057050e+00  5.92521269e+00 -7.22470732e-03 -9.67992974e-01\n",
    "#      1.70448714e-01 -9.38971600e-03 -3.92421893e-01  1.49830571e-02\n",
    "#     -4.16973012e-01]\n",
    "#    [-9.16297843e-02  4.86751203e-02 -3.77930006e-03  2.85636751e+00\n",
    "#     -2.88077933e+00  5.92521432e+00 -7.22447929e-03 -9.67995240e-01\n",
    "#      1.70443393e-01 -9.38925373e-03 -3.92425680e-01  1.49832102e-02\n",
    "#     -4.16972624e-01]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Loss function diagrams**\n",
    "___\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}