{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Introduction**\n",
    "___\n",
    "- Tidy data\n",
    "    - every column is a feature\n",
    "    - every row is an observation for each variable\n",
    "    - Pandas dataframe .shape attribute\n",
    "- high dimensionality > 10 columns\n",
    "- When to use dimensionality reduction?\n",
    "    - drop columns with no variance (i.e. same values)\n",
    "    - Pandas dataframe .describe() method\n",
    "        - no variance = std = 0, max and min are the same\n",
    "        - exclude = 'number' --> will show information for non-numeric values\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Removing features without variance\n",
    "\n",
    "#A sample of the Pokemon dataset has been loaded as pokemon_df. To\n",
    "#get an idea of which features have little variance you should use\n",
    "#the IPython Shell to calculate summary statistics on this sample.\n",
    "#Then adjust the code to create a smaller, easier to understand,\n",
    "#dataset.\n",
    "\n",
    "# Leave this list as is\n",
    "number_cols = ['HP', 'Attack', 'Defense']\n",
    "\n",
    "# Remove the feature without variance from this list\n",
    "non_number_cols = ['Name', 'Type']\n",
    "\n",
    "# Create a new dataframe by subselecting the chosen features\n",
    "#df_selected = pokemon_df[number_cols + non_number_cols]\n",
    "\n",
    "# Prints the first 5 lines of the new dataframe\n",
    "#print(df_selected.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       HP  Attack  Defense                   Name   Type\n",
    "#    0  45      49       49              Bulbasaur  Grass\n",
    "#    1  60      62       63                Ivysaur  Grass\n",
    "#    2  80      82       83               Venusaur  Grass\n",
    "#    3  80     100      123  VenusaurMega Venusaur  Grass\n",
    "#    4  39      52       43             Charmander   Fire\n",
    "#################################################\n",
    "#All Pokemon in this dataset are non-legendary and from generation\n",
    "#one so you could choose to drop those two features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Feature selection vs feature extraction**\n",
    "___\n",
    "- Why reduce dimensionality?\n",
    "    - your dataset will:\n",
    "        - be less complex\n",
    "        - require less disk space\n",
    "        - have lower chance of model overfitting\n",
    "- Feature selection\n",
    "    - .drop('column name', axis=1) [axis indicates column instead of row]\n",
    "- Building a pairplot\n",
    "    - sns.pairplot(data, hue='', diag_kind='hist')\n",
    "- Feature extraction\n",
    "    - calculating new feature(s) from original feature(s)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visually detecting redundant features\n",
    "#Data visualization is a crucial step in any data exploration. Let's\n",
    "#use Seaborn to explore some samples of the US Army ANSUR body\n",
    "#measurement dataset.\n",
    "\n",
    "#Two data samples have been pre-loaded as ansur_df_1 and ansur_df_2.\n",
    "\n",
    "#Seaborn has been imported as sns.\n",
    "\n",
    "# Create a pairplot and color the points using the 'Gender' feature\n",
    "#sns.pairplot(ansur_df_1, hue='Gender', diag_kind='hist')\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.1.svg](_images/13.1.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Two features are basically duplicates, remove one of them from\n",
    "#the dataset.\n",
    "\n",
    "# Remove one of the redundant features\n",
    "#reduced_df = ansur_df_1.drop('stature_m', axis=1)\n",
    "\n",
    "# Create a pairplot and color the points using the 'Gender' feature\n",
    "#sns.pairplot(reduced_df, hue='Gender')\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.2.svg](_images/13.2.svg)\n",
    "the body height (inches) and stature (meters) hold the same information in a different unit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Now create a pairplot of the ansur_df_2 data sample and color the\n",
    "#points using the 'Gender' feature.\n",
    "\n",
    "# Create a pairplot and color the points using the 'Gender' feature\n",
    "#sns.pairplot(ansur_df_2, hue='Gender', diag_kind='hist')\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.3.svg](_images/13.3.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#One feature has no variance, remove it from the dataset.\n",
    "# Remove the redundant feature\n",
    "#reduced_df = ansur_df_2.drop('n_legs', axis=1)\n",
    "\n",
    "# Create a pairplot and color the points using the 'Gender' feature\n",
    "#sns.pairplot(reduced_df, hue='Gender', diag_kind='hist')\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.4.svg](_images/13.4.svg)\n",
    "all the individuals in the second sample have two legs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**t-SNE visualization of high-dimensional data**\n",
    "___\n",
    "- t-distributed stochastic neighbor embedding\n",
    "- t-SNE maximizes distance in 2-dimensional space between dimensions in higher dimensional space\n",
    "- does not work with non-numeric values\n",
    "- learning rate (10-1000) - lower number is conservative\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Fitting t-SNE to the ANSUR data\n",
    "\n",
    "#t-SNE is a great technique for visual exploration of high dimensional\n",
    "#datasets. In this exercise, you'll apply it to the ANSUR dataset. You'll\n",
    "#remove non-numeric columns from the pre-loaded dataset df and fit TSNE to\n",
    "#this numeric dataset.\n",
    "\n",
    "# Non-numerical columns in the dataset\n",
    "#non_numeric = ['Branch', 'Gender', 'Component']\n",
    "\n",
    "# Drop the non-numerical columns from df\n",
    "#df_numeric = df.drop(non_numeric, axis=1)\n",
    "\n",
    "# Create a t-SNE model with learning rate 50\n",
    "#m = TSNE(learning_rate=50)\n",
    "\n",
    "# Fit and transform the t-SNE model on the numeric dataset\n",
    "#tsne_features = m.fit_transform(df_numeric)\n",
    "\n",
    "#################################################\n",
    "#t-SNE reduced the more than 90 features in the dataset to just 2\n",
    "#which you can now plot.\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#t-SNE visualisation of dimensionality\n",
    "#Time to look at the results of your hard work. In this exercise,\n",
    "#you will visualize the output of t-SNE dimensionality reduction on\n",
    "#the combined male and female Ansur dataset. You'll create 3\n",
    "#scatterplots of the 2 t-SNE features ('x' and 'y') which were\n",
    "#added to the dataset df. In each scatterplot you'll color the\n",
    "#points according to a different categorical variable.\n",
    "\n",
    "#seaborn has already been imported as sns and matplotlib.pyplot as\n",
    "#plt.\n",
    "\n",
    "# Color the points according to Army Component\n",
    "#sns.scatterplot(x=\"x\", y=\"y\", hue='Component', data=df)\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.5.svg](_images/13.5.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Color the points by Army Branch\n",
    "#sns.scatterplot(x=\"x\", y=\"y\", hue='Branch', data=df)\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.6.svg](_images/13.6.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Color the points by Gender\n",
    "#sns.scatterplot(x=\"x\", y=\"y\", hue='Gender', data=df)\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.7.svg](_images/13.7.svg)\n",
    "There is a Male and a Female cluster. t-SNE found these gender\n",
    "differences in body shape without being told about them explicitly!\n",
    "\n",
    "From the second plot you learned there are more males in the\n",
    "Combat Arms Branch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**The curse of dimensionality**\n",
    "___\n",
    "- as number of features increase in order to better fit a model, the number of observations must increase exponentially\n",
    "___\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train - test split\n",
    "#In this chapter, you will keep working with the ANSUR dataset.\n",
    "#Before you can build a model on your dataset, you should first\n",
    "#decide on which feature you want to predict. In this case, you're\n",
    "#trying to predict gender.\n",
    "\n",
    "#You need to extract the column holding this feature from the\n",
    "#dataset and then split the data into a training and test set. The\n",
    "#training set will be used to train the model and the test set will\n",
    "#be used to check its performance on unseen data.\n",
    "\n",
    "#ansur_df has been pre-loaded for you.\n",
    "\n",
    "# Import train_test_split()\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select the Gender column as the feature to be predicted (y)\n",
    "#y = ansur_df['Gender']\n",
    "\n",
    "# Remove the Gender column to create the training data\n",
    "#X = ansur_df.drop('Gender', axis=1)\n",
    "\n",
    "# Perform a 70% train and 30% test data split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "#print(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    300 rows in test set vs. 700 in training set. 91 Features.\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Fitting and testing the model\n",
    "\n",
    "#In the previous exercise, you split the dataset into X_train,\n",
    "#X_test, y_train, and y_test. These datasets have been pre-loaded\n",
    "#for you. You'll now create a support vector machine classifier\n",
    "#model (SVC()) and fit that to the training data. You'll then\n",
    "#calculate the accuracy on both the test and training set to detect\n",
    "#overfitting.\n",
    "\n",
    "# Import SVC from sklearn.svm and accuracy_score from sklearn.metrics\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an instance of the Support Vector Classification class\n",
    "#svc = SVC()\n",
    "\n",
    "# Fit the model to the training data\n",
    "#svc.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy scores on both train and test data\n",
    "#accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
    "#accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "#print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    49.7% accuracy on test set vs. 100.0% on training set\n",
    "#################################################\n",
    "#Looks like the model badly overfits on the training data. On unseen\n",
    "#data it performs worse than a random selector would."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Accuracy after dimensionality reduction\n",
    "\n",
    "#You'll reduce the overfit with the help of dimensionality reduction.\n",
    "#In this case, you'll apply a rather drastic form of dimensionality\n",
    "#reduction by only selecting a single column that has some good\n",
    "#information to distinguish between genders. You'll repeat the\n",
    "#train-test split, model fit and prediction steps to compare the\n",
    "#accuracy on test vs. training data.\n",
    "\n",
    "#All relevant packages and y have been pre-loaded.\n",
    "\n",
    "# Assign just the 'neckcircumferencebase' column from ansur_df to X\n",
    "#X = ansur_df[['neckcircumferencebase']]\n",
    "\n",
    "# Split the data, instantiate a classifier and fit the data\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "#svc = SVC()\n",
    "#svc.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy scores on both train and test data\n",
    "#accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
    "#accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "#print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#   93.3% accuracy on test set vs. 94.9% on training set\n",
    "#################################################\n",
    "#On the full dataset the model is rubbish but with a single feature\n",
    "#we can make good predictions? This is an example of the curse of\n",
    "#dimensionality! The model badly overfits when we feed it too many\n",
    "#features. It overlooks that neck circumference by itself is pretty\n",
    "#different for males and females."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Features with missing values or little variance**\n",
    "___\n",
    "- Variance thresholds are not always easy to interpret or compare between features\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Finding a good variance threshold\n",
    "\n",
    "#You'll be working on a slightly modified subsample of the ANSUR\n",
    "#dataset with just head measurements pre-loaded as head_df.\n",
    "\n",
    "#Create a boxplot on head_df.\n",
    "\n",
    "# Create the boxplot\n",
    "#head_df.boxplot()\n",
    "\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.8.svg](_images/13.8.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Normalize the data by dividing the dataframe with its mean values.\n",
    "\n",
    "# Normalize the data\n",
    "#normalized_df = head_df / head_df.mean()\n",
    "\n",
    "#normalized_df.boxplot()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.9.svg](_images/13.9.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Print the variances of the normalized data.\n",
    "\n",
    "# Normalize the data\n",
    "#normalized_df = head_df / head_df.mean()\n",
    "\n",
    "# Print the variances of the normalized data\n",
    "#print(normalized_df.var())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    headbreadth          1.678952e-03\n",
    "#    headcircumference    1.029623e-03\n",
    "#    headlength           1.867872e-03\n",
    "#    tragiontopofhead     2.639840e-03\n",
    "#    n_hairs              1.002552e-08\n",
    "#    measurement_error    3.231707e-27\n",
    "#    dtype: float64\n",
    "#################################################\n",
    "#Q: If you want to remove the 2 very low variance features.\n",
    "#What would be a good variance threshold?\n",
    "#A: 1.0e-03"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Features with low variance\n",
    "\n",
    "#In the previous exercise you established that 0.001 is a good\n",
    "#threshold to filter out low variance features in head_df after\n",
    "#normalization. Now use the VarianceThreshold feature selector to\n",
    "#remove these features.\n",
    "\n",
    "#from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create a VarianceThreshold feature selector\n",
    "#sel = VarianceThreshold(threshold=0.001)\n",
    "\n",
    "# Fit the selector to normalized head_df\n",
    "#sel.fit(head_df / head_df.mean())\n",
    "\n",
    "# Create a boolean mask\n",
    "#mask = sel.get_support()\n",
    "\n",
    "# Apply the mask to create a reduced dataframe\n",
    "#reduced_df = head_df.loc[:, mask]\n",
    "\n",
    "#print(\"Dimensionality reduced from {} to {}.\".format(head_df.shape[1], reduced_df.shape[1]))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Dimensionality reduced from 6 to 4.\n",
    "#################################################\n",
    "#you've successfully removed the 2 low-variance features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Removing features with many missing values\n",
    "\n",
    "#You'll apply feature selection on the Boston Public Schools\n",
    "#dataset which has been pre-loaded as school_df. Calculate the\n",
    "#missing value ratio per feature and then create a mask to remove\n",
    "#features with many missing values.\n",
    "\n",
    "#school_df.isna().sum() / len(school_df)\n",
    "#################################################\n",
    "#x             0.000000\n",
    "#y             0.000000\n",
    "#objectid_1    0.000000\n",
    "#objectid      0.000000\n",
    "#bldg_id       0.000000\n",
    "#bldg_name     0.000000\n",
    "#address       0.000000\n",
    "#city          0.000000\n",
    "#zipcode       0.000000\n",
    "#csp_sch_id    0.000000\n",
    "#sch_id        0.000000\n",
    "#sch_name      0.000000\n",
    "#sch_label     0.000000\n",
    "#sch_type      0.000000\n",
    "#shared        0.877863\n",
    "#complex       0.984733\n",
    "#label         0.000000\n",
    "#tlt           0.000000\n",
    "#pl            0.000000\n",
    "#point_x       0.000000\n",
    "#point_y       0.000000\n",
    "#dtype: float64\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Create a boolean mask on whether each feature has less than 50%\n",
    "#missing values.\n",
    "\n",
    "#Apply the mask to school_df to select columns without many missing\n",
    "#values.\n",
    "\n",
    "# Create a boolean mask on whether each feature less than 50% missing values.\n",
    "#mask = school_df.isna().sum() / len(school_df) < 0.5\n",
    "\n",
    "# Create a reduced dataset by applying the mask\n",
    "#reduced_df = school_df.loc[:, mask]\n",
    "\n",
    "#print(school_df.shape)\n",
    "#print(reduced_df.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (131, 21)\n",
    "#    (131, 19)\n",
    "#################################################\n",
    "#The number of features went down from 21 to 19."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pairwise correlation**\n",
    "___\n",
    "- pairplots\n",
    "- correlation coefficient\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visualizing the correlation matrix\n",
    "\n",
    "#Reading the correlation matrix of ansur_df in its raw, numeric\n",
    "#format doesn't allow us to get a quick overview. Let's improve\n",
    "#this by removing redundant values and visualizing the matrix using\n",
    "#seaborn.\n",
    "\n",
    "#Seaborn has been pre-loaded as sns, matplotlib.pyplot as plt,\n",
    "#NumPy as np and pandas as pd.\n",
    "\n",
    "# Create the correlation matrix\n",
    "#corr = ansur_df.corr()\n",
    "\n",
    "# Draw the heatmap\n",
    "#sns.heatmap(corr,  cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.10.svg](_images/13.10.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Create a boolean mask for the upper triangle of the plot.\n",
    "\n",
    "# Create the correlation matrix\n",
    "#corr = ansur_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "#mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Add the mask to the heatmap\n",
    "#sns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.11.svg](_images/13.11.svg)\n",
    "The buttock and crotch height have a 0.93 correlation coefficient."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Removing highly correlated features**\n",
    "___\n",
    "- correlation caveats - Anscombe's quartet\n",
    "    - nonlinear relationships or datasets with outliers may also correlate strongly\n",
    "    - always visualize the scatterplot\n",
    "- correlation does not imply causation\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Filtering out highly correlated features\n",
    "#You're going to automate the removal of highly correlated features\n",
    "#in the numeric ANSUR dataset. You'll calculate the correlation\n",
    "#matrix and filter out columns that have a correlation coefficient\n",
    "#of more than 0.95 or less than -0.95.\n",
    "\n",
    "#Since each correlation coefficient occurs twice in the matrix\n",
    "#(correlation of A to B equals correlation of B to A) you'll want\n",
    "#to ignore half of the correlation matrix so that only one of the\n",
    "#two correlated features is removed. Use a mask trick for this\n",
    "#purpose.\n",
    "\n",
    "# Calculate the correlation matrix and take the absolute value\n",
    "#corr_matrix = ansur_df.corr().abs()\n",
    "\n",
    "# Create a True/False mask and apply it\n",
    "#mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "#tri_df = corr_matrix.mask(mask)\n",
    "\n",
    "# List column names of highly correlated features (r > 0.95)\n",
    "#to_drop = [c for c in tri_df.columns if any(tri_df[c] > 0.95)]\n",
    "\n",
    "# Drop the features in the to_drop list\n",
    "#reduced_df = ansur_df.drop(to_drop, axis=1)\n",
    "\n",
    "#print(\"The reduced_df dataframe has {} columns\".format(reduced_df.shape[1]))\n",
    "\n",
    "#################################################\n",
    "#The original dataframe has 99 columns.\n",
    "#\n",
    "#<script.py> output:\n",
    "#    The reduced_df dataframe has 88 columns\n",
    "#################################################\n",
    "# You've automated the removal of highly correlated features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Nuclear energy and pool drownings\n",
    "\n",
    "#The dataset that has been pre-loaded for you as weird_df contains\n",
    "#actual data provided by the US Centers for Disease Control &\n",
    "#Prevention and Department of Energy.\n",
    "\n",
    "#Let's see if we can find a pattern.\n",
    "\n",
    "#Seaborn has been pre-loaded as sns and matplotlib.pyplot as plt.\n",
    "\n",
    "# Print the first five lines of weird_df\n",
    "#print(weird_df.head())\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       pool_drownings  nuclear_energy\n",
    "#    0             421           728.3\n",
    "#    1             465           753.9\n",
    "#    2             494           768.8\n",
    "#    3             538           780.1\n",
    "#    4             430           763.7\n",
    "#################################################\n",
    "\n",
    "#Create a scatterplot with nuclear energy production on the x-axis\n",
    "#and the number of pool drownings on the y-axis.\n",
    "\n",
    "sns.scatterplot(x='nuclear_energy', y='pool_drownings', data=weird_df)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.12.svg](_images/13.12.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print out the correlation matrix of weird_df\n",
    "print(weird_df.corr())\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#                    pool_drownings  nuclear_energy\n",
    "#    pool_drownings        1.000000        0.901179\n",
    "#    nuclear_energy        0.901179        1.000000\n",
    "#################################################\n",
    "#While the example is silly, you'll be amazed how often people\n",
    "#misunderstand correlation vs causation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Selecting features for model performance**\n",
    "___\n",
    "\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}