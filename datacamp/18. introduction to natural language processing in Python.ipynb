{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Introduction to regular expressions**\n",
    "___\n",
    "- What is Natural Language Processing?\n",
    "    - field of study focused on making sense of language\n",
    "        - using statistics and computers\n",
    "    - you will learn the basics of NLP\n",
    "        - topic identification\n",
    "        - text classification\n",
    "    - other NLP applications\n",
    "        - chatbots\n",
    "        - translation\n",
    "        - sentiment analysis\n",
    "- What exactly are regular expressions?\n",
    "    - strings with a special syntax\n",
    "    - allow us to match patterns in other strings\n",
    "    - applications of regular expressions:\n",
    "        - find all web links in a document\n",
    "        - parse email addresses\n",
    "        - remove/replace unwanted characters\n",
    "- in Python\n",
    "    - import re\n",
    "    - re.match('pattern', 'string')\n",
    "![_images/18.1.PNG](_images/18.1.PNG)\n",
    "![_images/18.2.PNG](_images/18.2.PNG)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "#Practicing regular expressions: re.split() and re.findall()\n",
    "\n",
    "#Now you'll get a chance to write some regular expressions to match\n",
    "#digits, strings and non-alphanumeric characters. Take a look at\n",
    "#my_string first by printing it in the IPython Shell, to determine\n",
    "#how you might best match the different steps.\n",
    "\n",
    "#Note: It's important to prefix your regex patterns with r to ensure\n",
    "#that your patterns are interpreted in the way you want them to. Else,\n",
    "#you may encounter problems to do with escape sequences in strings. For\n",
    "#example, \"\\n\" in Python is used to indicate a new line, but if you use\n",
    "#the r prefix, it will be interpreted as the raw string \"\\n\" - that is,\n",
    "#the character \"\\\" followed by the character \"n\" - and not as a new line.\n",
    "\n",
    "#The regular expression module re has already been imported for you.\n",
    "\n",
    "#Remember from the video that the syntax for the regex library is to\n",
    "#always to pass the pattern first, and then the string second.\n",
    "\n",
    "import re\n",
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))\n",
    "\n",
    "#################################################\n",
    "#Practice is the key to mastering RegEx."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Introduction to tokenization**\n",
    "___\n",
    "- What is tokenization?\n",
    "    - turning a string or document into **tokens** (smaller chunks)\n",
    "    - one step in preparing a text for NLP\n",
    "    - many different theories and rules\n",
    "    - you can create your own rules using regular expressions\n",
    "    - some examples:\n",
    "        - breaking our words or sentences\n",
    "        - separating punctuation\n",
    "        - separating all hashtags in a tweet\n",
    "    - Why tokenize?\n",
    "        - easier to map part of the speech\n",
    "        - matching common words\n",
    "        - removing unwanted tokens\n",
    "    - nltk library\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beat', 'under', 'winter', 'matter', 'kingdom', 'Oh', 'It', 'suggesting', 'minute', 'King', 'SCENE', 'halves', 'get', 'they', 'use', 'house', 'Patsy', 'coconut', \"'em\", 'They', 'together', \"'s\", 'course', 'sovereign', 'since', 'covered', 'I', \"n't\", 'five', 'speak', 'zone', 'and', 'point', 'strand', 'why', 'horse', 'Whoa', 'in', '.', 'In', 'Well', 'or', 'England', '1', 'Halt', 'here', \"'re\", 'Where', 'What', 'to', 'times', 'one', 'weight', 'bird', 'he', 'carrying', 'creeper', 'master', 'who', 'bring', 'ounce', 'Found', 'this', 'lord', 'at', 'coconuts', 'No', 'tropical', 'Please', 'goes', 'martin', 'other', 'African', 'through', 'seek', 'maintain', 'Mercea', 'Not', 'join', 'bangin', 'be', 'ask', 'non-migratory', 'anyway', 'grips', 'by', 'will', 'am', 'does', 'migrate', 'empty', 'KING', '[', 'plover', 'an', 'you', 'carry', 'found', 'are', 'We', 'could', 'fly', 'but', 'Yes', 'it', 'Will', 'mean', 'You', 'just', \"'d\", 'Ridden', 'swallow', 'simple', 'tell', '!', 'these', 'my', '--', '#', 'Arthur', 'yet', 'swallows', 'climes', 'pound', 'castle', 'Am', 'temperate', 'husk', 'is', 'then', 'second', 'dorsal', 'ratios', 'may', 'held', ']', 'me', 'a', 'Listen', 'air-speed', 'SOLDIER', 'son', 'back', 'search', 'Britons', 'breadth', 'ridden', 'wings', ',', 'two', 'if', 'guiding', 'servant', 'strangers', 'land', 'Wait', 'on', 'question', 'Who', '?', 'A', 'go', 'interested', 'order', 'using', 'carried', \"'\", ':', 'that', '2', 'But', 'clop', 'do', 'with', 'ARTHUR', 'The', 'the', 'from', 'your', 'snows', 'court', 'of', 'Uther', 'length', 'So', 'needs', 'That', 'wants', 'Saxons', 'Camelot', \"'m\", 'them', 'European', 'have', \"'ve\", 'maybe', 'line', 'sun', 'where', 'Are', 'there', 'not', 'all', 'grip', 'velocity', 'got', '...', 'wind', 'must', 'Court', 'trusty', 'agree', 'right', 'south', 'warmer', 'our', 'feathers', 'Pull', 'defeator', 'knights', 'every', 'yeah', 'Supposing', 'Pendragon', 'forty-three', 'its'}\n"
     ]
    }
   ],
   "source": [
    "#Word tokenization with NLTK\n",
    "\n",
    "#Here, you'll be using the first scene of Monty Python's Holy Grail,\n",
    "#which has been pre-loaded as scene_one. Feel free to check it out in\n",
    "#the IPython Shell!\n",
    "\n",
    "#Your job in this exercise is to utilize word_tokenize and sent_tokenize\n",
    "#from nltk.tokenize to tokenize both words and sentences from Python\n",
    "#strings - in this case, the first scene of Monty Python's Holy Grail.\n",
    "\n",
    "scene_one=\"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\"\n",
    "\n",
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 588\n",
      "<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n",
      "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "#More regex with re.search()\n",
    "\n",
    "#In this exercise, you'll utilize re.search() and re.match() to find\n",
    "#specific tokens. Both search and match expect regex patterns, similar\n",
    "#to those you defined in an earlier exercise. You'll apply these regex\n",
    "#library methods to the same Monty Python text from the nltk corpora.\n",
    "\n",
    "#You have both scene_one and sentences available from the last exercise;\n",
    "#now you can use them with re.search() and re.match() to extract and\n",
    "#match more text.\n",
    "\n",
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "scene_one=\"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\"\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))\n",
    "\n",
    "#################################################\n",
    "#Now that you're familiar with the basics of tokenization and\n",
    "#regular expressions, it's time to learn about more advanced tokenization."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Advanced tokenization with NLTK and regex**\n",
    "___\n",
    "- Regex groups using the \"|\"\n",
    "    - OR is represented using |\n",
    "    - You can define a group using ()\n",
    "    - You can define explicit character ranges using []\n",
    "- Regex ranges and groups\n",
    "![_images/18.3.PNG](_images/18.3.PNG)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python']\n",
      "['@datacamp', '#nlp', '#python']\n",
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "#Regex with NLTK tokenization\n",
    "\n",
    "#Twitter is a frequently used source for NLP text and tasks. In this\n",
    "#exercise, you'll build a more complex tokenizer for tweets with\n",
    "#hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer\n",
    "#class gives you some extra methods and attributes for parsing tweets.\n",
    "\n",
    "#Here, you're given some example tweets to parse using both\n",
    "#TweetTokenizer and regexp_tokenize from the nltk.tokenize module.\n",
    "#These example tweets have been pre-loaded into the variable tweets.\n",
    "#Feel free to explore it in the IPython Shell!\n",
    "\n",
    "#Unlike the syntax for the regex library, with nltk_tokenize() you\n",
    "#pass the pattern as the second argument.\n",
    "\n",
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)\n",
    "\n",
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([@#]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[2], pattern2)\n",
    "print(mentions_hashtags)\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n",
      "['Wann', 'Pizza', 'Und', 'Über']\n",
      "['🍕', '🚕']\n"
     ]
    }
   ],
   "source": [
    "#Non-ascii tokenization\n",
    "\n",
    "#In this exercise, you'll practice advanced tokenization by tokenizing\n",
    "#some non-ascii based text. You'll be using German with emoji!\n",
    "\n",
    "#Here, you have access to a string called german_text, which has\n",
    "#been printed for you in the Shell. Notice the emoji and the German\n",
    "#characters!\n",
    "\n",
    "#The following modules have been pre-imported from nltk.tokenize:\n",
    "#regexp_tokenize and word_tokenize.\n",
    "\n",
    "#Unicode ranges for emoji are:\n",
    "\n",
    "#('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'),\n",
    "#('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF').\n",
    "\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "german_text = 'Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕'\n",
    "\n",
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÜ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Charting word length with NLTK**\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Charting practice\n",
    "\n",
    "#Try using your new skills to find and chart the number of words per\n",
    "#line in the script using matplotlib. The Holy Grail script is loaded\n",
    "#for you, and you need to use regex to find the words per line.\n",
    "\n",
    "#Using list comprehensions here will speed up your computations. For\n",
    "#example: my_lines = [tokenize(l) for l in lines] will call a function\n",
    "#tokenize on each line in the list lines. The new transformed list\n",
    "#will be saved in the my_lines variable.\n",
    "\n",
    "#You have access to the entire script in the variable holy_grail.\n",
    "#Go for it!\n",
    "\n",
    "# Split the script into lines: lines\n",
    "#lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "#pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "#lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "#tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "#line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "#plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/18.1.svg](_images/18.1.svg)\n",
    "See you in Chapter 2, where you'll begin learning about topic identification!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Word counts with bag-of-words**\n",
    "___\n",
    "- first create tokens using tokenization\n",
    "- count all of the tokens\n",
    "- the more frequent a word, the more important it might be\n",
    "___"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Building a Counter with bag-of-words\n",
    "\n",
    "#In this exercise, you'll build your first (in this course)\n",
    "#bag-of-words counter using a Wikipedia article, which has been\n",
    "#pre-loaded as article. Try doing the bag-of-words without looking\n",
    "#at the full article text, and guessing what the topic is! If you'd\n",
    "#like to peek at the title at the end, we've included it as\n",
    "#article_title. Note that this article text has had very little\n",
    "#preprocessing from the raw Wikipedia database entry.\n",
    "\n",
    "#word_tokenize has been imported for you.\n",
    "\n",
    "# Import Counter\n",
    "#from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "#tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "#lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "#bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "#print(bow_simple.most_common(10))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 68), ('to', 63), ('a', 60), ('in', 44), ('and', 41), ('debugging', 40)]\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Simple text preprocessing**\n",
    "___\n",
    "- Why preprocess?\n",
    "    - helps make for better input data\n",
    "        - when performing machine learning or other statistical methods\n",
    "    - examples:\n",
    "        - tokenization to create bag of words\n",
    "        - lowercasing words\n",
    "    - lemmatization/stemming\n",
    "        - shorten words to their root stems\n",
    "    - removing stop words, punctuation, or unwanted tokens\n",
    "    - good to experiment with different approaches\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Text preprocessing practice\n",
    "\n",
    "#Now, it's your turn to apply the techniques you've learned to help\n",
    "#clean up text for better NLP results. You'll need to remove stop\n",
    "#words and non-alphabetic characters, lemmatize, and perform a new\n",
    "#bag-of-words on your cleaned text.\n",
    "\n",
    "#You start with the same tokens you created in the last exercise:\n",
    "#lower_tokens. You also have the Counter class imported.\n",
    "\n",
    "# Import WordNetLemmatizer\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "#alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "#no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "#wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "#lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "#bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "#print(bow.most_common(10))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [('debugging', 40), ('system', 25), ('software', 16), ('bug', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('used', 12)]\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Introduction to gensim**\n",
    "___\n",
    "- What is gensim?\n",
    "    - popular open-source NLP library\n",
    "- Uses top academic models to perform complex tasks\n",
    "    - building a document or words vectors\n",
    "    - performing topic identification and document comparison\n",
    "![_images/18.4.PNG](_images/18.4.PNG)\n",
    "![_images/18.5.PNG](_images/18.5.PNG)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Creating and querying a corpus with gensim\n",
    "\n",
    "#It's time to apply the methods you learned in the previous video to\n",
    "#create your first gensim dictionary and corpus!\n",
    "\n",
    "#You'll use these data structures to investigate word trends and\n",
    "#potential interesting topics in your document set. To get started,\n",
    "#we have imported a few additional messy articles from Wikipedia,\n",
    "#which were preprocessed by lowercasing all words, tokenizing them,\n",
    "#and removing stop words and punctuation. These were then stored in\n",
    "#a list of document tokens called articles. You'll need to do some\n",
    "#light preprocessing and then generate the gensim dictionary and corpus.\n",
    "\n",
    "# Import Dictionary\n",
    "#from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "#dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "#computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "#print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "#corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "#print(corpus[4][:10])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    computer\n",
    "#    [(0, 88), (23, 11), (24, 2), (39, 1), (41, 2), (55, 22), (56, 1), (57, 1), (58, 1), (59, 3)]\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Gensim bag-of-words\n",
    "\n",
    "#Now, you'll use your new gensim corpus and dictionary to see the\n",
    "#most common terms per document and across all documents. You can\n",
    "#use your dictionary to look up the terms. Take a guess at what the\n",
    "#topics are and feel free to explore more documents in the IPython\n",
    "#Shell!\n",
    "\n",
    "#You have access to the dictionary and corpus objects you created in\n",
    "#the previous exercise, as well as the Python defaultdict and itertools\n",
    "#to help with the creation of intermediate data structures for analysis.\n",
    "\n",
    "#defaultdict allows us to initialize a dictionary that will assign\n",
    "#a default value to non-existent keys. By supplying the argument\n",
    "#int, we are able to ensure that any non-existent keys are automatically\n",
    "#assigned a default value of 0. This makes it ideal for storing the\n",
    "#counts of words in this exercise.\n",
    "\n",
    "#itertools.chain.from_iterable() allows us to iterate through a set\n",
    "#of sequences as if they were one continuous sequence. Using this\n",
    "#function, we can easily iterate through our corpus object (which is\n",
    "#a list of lists).\n",
    "\n",
    "#The fifth document from corpus is stored in the variable doc, which\n",
    "#has been sorted in descending order.\n",
    "\n",
    "# Save the fifth document: doc\n",
    "#doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "#bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "#for word_id, word_count in bow_doc[:5]:\n",
    "#    print(dictionary.get(word_id), word_count)\n",
    "\n",
    "# Create the defaultdict: total_word_count\n",
    "#total_word_count = defaultdict(int)\n",
    "#for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "#    total_word_count[word_id] += word_count\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    engineering 91\n",
    "#    '' 88\n",
    "#    reverse 71\n",
    "#    software 51\n",
    "#    cite 26\n",
    "#################################################\n",
    "\n",
    "# Create the defaultdict: total_word_count\n",
    "#total_word_count = defaultdict(int)\n",
    "#for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "#    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "#sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "#for word_id, word_count in sorted_word_count[:5]:\n",
    "#    print(dictionary.get(word_id), word_count)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    engineering 91\n",
    "#    '' 88\n",
    "#    reverse 71\n",
    "#    software 51\n",
    "#    cite 26\n",
    "#    '' 1042\n",
    "#    computer 594\n",
    "#    software 450\n",
    "#    `` 345\n",
    "#    cite 322\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Tf-idf with gensim**\n",
    "___\n",
    "- What is tf-idf?\n",
    "    - term frequency-inverse document frequency\n",
    "    - allows you to determine the most important words in each document\n",
    "    - each corpus nay have shared words beyond just stopwords\n",
    "    - these words should be down-weighted in importance\n",
    "    - ensures most common words do not show up as key words\n",
    "    - keeps document specific frequent words weighted high\n",
    "![_images/18.6.PNG](_images/18.6.PNG)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Tf-idf with Wikipedia\n",
    "\n",
    "#Now it's your turn to determine new significant terms for your\n",
    "#corpus by applying gensim's tf-idf. You will again have access to\n",
    "#the same corpus and dictionary objects you created in the previous\n",
    "#exercises - dictionary, corpus, and doc. Will tf-idf make for more\n",
    "#interesting results on the document level?\n",
    "\n",
    "#TfidfModel has been imported for you from gensim.models.tfidfmodel.\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "#tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "#tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "#print(tfidf_weights[:5])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [(24, 0.0022836332291091273), (39, 0.0043409401554717324), (41, 0.008681880310943465), (55, 0.011988285029371418), (56, 0.005482756770026296)]\n",
    "#################################################\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "#sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "#for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "#    print(dictionary.get(term_id), weight)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    reverse 0.4884961428651127\n",
    "#    infringement 0.18674529210288995\n",
    "#    engineering 0.16395041814479536\n",
    "#    interoperability 0.12449686140192663\n",
    "#    reverse-engineered 0.12449686140192663\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}