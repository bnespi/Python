{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**What is data preprocessing?**\n",
    "___\n",
    "- beyond cleaning and exploratory data analysis\n",
    "- prepping data for modeling\n",
    "    e.g. transforming categorical data to numeric\n",
    "- Pandas\n",
    "    - .columns\n",
    "    - .dtypes\n",
    "    - .describe()\n",
    "    - remove missing data\n",
    "        - .dropna() - drop rows with NA values (axis=0, thresh=1)\n",
    "        - df[\"B\"].isnull().sum() - sum of all null values for specific column\n",
    "        - df[df[\"B\"].notnull()] - index for values that are not null for specific columns\n",
    "    - .drop([1, 2, 3]) - drop specific rows\n",
    "    - .drop(\"A\", axis = 1) - drop specific columns\n",
    "    - df[df[\"B\"] == 7] - boolean indexing of columns\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Missing data - rows\n",
    "\n",
    "#Taking a look at the volunteer dataset again, we want to drop rows\n",
    "#where the category_desc column values are missing. We're going to do\n",
    "#this using boolean indexing, by checking to see if we have any null\n",
    "#values, and then filtering the dataset so that we only have rows\n",
    "#with those values.\n",
    "\n",
    "# Check how many values are missing in the category_desc column\n",
    "#print(volunteer[\"category_desc\"].isnull().sum())\n",
    "\n",
    "# Subset the volunteer dataset\n",
    "#volunteer_subset = volunteer[volunteer[\"category_desc\"].notnull()]\n",
    "\n",
    "# Print out the shape of the subset\n",
    "#print(volunteer_subset.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    48\n",
    "#    (617, 35)\n",
    "#################################################\n",
    "#Remember that you can use boolean indexing to effectively subset\n",
    "#DataFrames."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Working with data types**\n",
    "___\n",
    "- Why are types important?\n",
    "    - .dtypes\n",
    "        - object - string/mixed types\n",
    "        - int64 - integer\n",
    "        - float64 - float\n",
    "        - datetime64 (or timedelta) - datetime\n",
    "- Converting column types\n",
    "    - .astype(\"float\")\n",
    "___"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Converting a column type\n",
    "\n",
    "#If you take a look at the volunteer dataset types, you'll see that\n",
    "#the column hits is type object. But, if you actually look at the\n",
    "#column, you'll see that it consists of integers. Let's convert that\n",
    "#column to type int.\n",
    "\n",
    "# Print the head of the hits column\n",
    "#print(volunteer[\"hits\"].head())\n",
    "\n",
    "# Convert the hits column to type int\n",
    "#volunteer[\"hits\"] = volunteer[\"hits\"].astype(\"int\")\n",
    "\n",
    "# Look at the dtypes of the dataset\n",
    "#print(volunteer.dtypes)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    737\n",
    "#    1     22\n",
    "#    2     62\n",
    "#    3     14\n",
    "#    4     31\n",
    "#\n",
    "#    Name: hits, dtype: object\n",
    "#    opportunity_id          int64\n",
    "#    content_id              int64\n",
    "#    vol_requests            int64\n",
    "#    event_time              int64\n",
    "#    title                  object\n",
    "#    hits                    int64\n",
    "#    summary                object\n",
    "#    is_priority            object\n",
    "#    category_id           float64\n",
    "#    category_desc          object\n",
    "#    amsl                  float64\n",
    "#    amsl_unit             float64\n",
    "#    org_title              object\n",
    "#    org_content_id          int64\n",
    "#    addresses_count         int64\n",
    "#    locality               object\n",
    "#    region                 object\n",
    "#    postalcode            float64\n",
    "#    primary_loc           float64\n",
    "#    display_url            object\n",
    "#    recurrence_type        object\n",
    "#    hours                   int64\n",
    "#    created_date           object\n",
    "#    last_modified_date     object\n",
    "#    start_date_date        object\n",
    "#    end_date_date          object\n",
    "#    status                 object\n",
    "#    Latitude              float64\n",
    "#    Longitude             float64\n",
    "#    Community Board       float64\n",
    "#    Community Council     float64\n",
    "#    Census Tract          float64\n",
    "#    BIN                   float64\n",
    "#    BBL                   float64\n",
    "#    NTA                   float64\n",
    "#    dtype: object\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Class distribution**\n",
    "___\n",
    "- How do you split train/test when your samples are not normally distributed?\n",
    "- Stratified sampling\n",
    "    - from train_test_split method .value_counts()\n",
    "    - parameter for train_test_split is \"stratify=\"\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Stratified sampling\n",
    "\n",
    "#We know that the distribution of variables in the category_desc\n",
    "#column in the volunteer dataset is uneven. If we wanted to train a\n",
    "#model to try to predict category_desc, we would want to train the\n",
    "#model on a sample of data that is representative of the entire\n",
    "#dataset. Stratified sampling is a way to achieve this.\n",
    "\n",
    "# Create a data with all columns except category_desc\n",
    "#volunteer_X = volunteer.drop(\"category_desc\", axis=1)\n",
    "\n",
    "# Create a category_desc labels dataset\n",
    "#volunteer_y = volunteer[[\"category_desc\"]]\n",
    "\n",
    "# Use stratified sampling to split up the dataset according to the volunteer_y dataset\n",
    "#X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y)\n",
    "\n",
    "# Print out the category_desc counts on the training y labels\n",
    "#print(y_train[\"category_desc\"].value_counts())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Strengthening Communities    230\n",
    "#    Helping Neighbors in Need     89\n",
    "#    Education                     69\n",
    "#    Health                        39\n",
    "#    Environment                   24\n",
    "#    Emergency Preparedness        11\n",
    "#    Name: category_desc, dtype: int64\n",
    "#    Strengthening Communities    230\n",
    "#    Helping Neighbors in Need     89\n",
    "#    Education                     69\n",
    "#    Health                        39\n",
    "#    Environment                   24\n",
    "#    Emergency Preparedness        11\n",
    "#    Name: category_desc, dtype: int64\n",
    "#################################################\n",
    "#ou'll use train_test_split frequently while building models, so\n",
    "#it's useful to be familiar with the function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Standardizing Data**\n",
    "___\n",
    "- scikit-learn models assume normally distributed data\n",
    "- applied to continuous numerical data\n",
    "- linearity assumptions\n",
    "- types discussed\n",
    "    - log normalization\n",
    "    - feature scaling\n",
    "- when to standardize models\n",
    "    - model in linear space\n",
    "    - dataset features have high variance\n",
    "    - dataset features are continuous and on different scales\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Modeling without normalizing\n",
    "\n",
    "#Let's take a look at what might happen to your model's accuracy if\n",
    "#you try to model data without doing some sort of standardization\n",
    "#first. Here we have a subset of the wine dataset. One of the\n",
    "#columns, Proline, has an extremely high variance compared to the\n",
    "#other columns. This is an example of where a technique like log\n",
    "#normalization would come in handy, which you'll learn about in\n",
    "#the next section.\n",
    "\n",
    "#The scikit-learn model training process should be familiar to you\n",
    "#at this point, so we won't go too in-depth with it. You already\n",
    "#have a k-nearest neighbors model available (knn) as well as the X\n",
    "#and y sets you need to fit and score on.\n",
    "\n",
    "# Split the dataset and labels into training and test sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "#knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "#print(knn.score(X_test, y_test))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.5333333333333333\n",
    "#################################################\n",
    "#You can see that the accuracy score is pretty low. Let's explore\n",
    "#methods to improve this score."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Log normalization**\n",
    "___\n",
    "- applies log transformation to feature(s) with high variance relative to other features\n",
    "- helps feature(s) approach normality\n",
    "- takes the log of e (2.718)\n",
    "    - e.g. log 30 = 3.4, log 300 = 5.7, log 3000 = 8\n",
    "- captures relative changes, the magnitude of change, and keeps everything in the positive space\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Log normalization in Python\n",
    "#Now that we know that the Proline column in our wine dataset has a\n",
    "#large amount of variance, let's log normalize it.\n",
    "\n",
    "#Numpy has been imported as np in your workspace.\n",
    "\n",
    "# Print out the variance of the Proline column\n",
    "#print(wine[\"Proline\"].var())\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "#wine[\"Proline_log\"] = np.log(wine[\"Proline\"])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "#print(wine[\"Proline_log\"].var())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    99166.71735542436\n",
    "#    0.17231366191842012\n",
    "#################################################\n",
    "#  The np.log() function is an easy way to log normalize a column."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Scaling data for feature comparison**\n",
    "___\n",
    "- What is feature scaling?\n",
    "    - features on different scales\n",
    "    - model with linear characteristics\n",
    "    - center features with mean of zero and transform unit variance to same\n",
    "    - transforms to approximately normal distribution\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Scaling data - standardizing columns\n",
    "\n",
    "#Since we know that the Ash, Alcalinity of ash, and Magnesium\n",
    "#columns in the wine dataset are all on different scales, let's\n",
    "#standardize them in a way that allows for use in a linear model.\n",
    "\n",
    "# Import StandardScaler from scikit-learn\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "#ss = StandardScaler()\n",
    "\n",
    "# Take a subset of the DataFrame you want to scale\n",
    "#wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\n",
    "\n",
    "# Apply the scaler to the DataFrame subset\n",
    "#wine_subset_scaled = ss.fit_transform(wine_subset)\n",
    "\n",
    "#################################################\n",
    "# In scikit-learn, running fit_transform during preprocessing will\n",
    "#both fit the method to the data as well as transform the data in a\n",
    "#single step."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Standardized data and modeling**\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#KNN on non-scaled data\n",
    "\n",
    "#Let's first take a look at the accuracy of a K-nearest neighbors\n",
    "#model on the wine dataset without standardizing the data. The knn\n",
    "#model as well as the X and y data and labels sets have been created\n",
    "#already. Most of this process of creating models in scikit-learn\n",
    "#should look familiar to you.\n",
    "\n",
    "# Split the dataset and labels into training and test sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "#knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "#print(knn.score(X_test, y_test))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.6444444444444445\n",
    "#################################################\n",
    "#This scikit-learn workflow should be very familiar to you at this point."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#KNN on scaled data\n",
    "\n",
    "#The accuracy score on the unscaled wine dataset was decent, but we\n",
    "#can likely do better if we scale the dataset. The process is mostly\n",
    "#the same as the previous exercise, with the added step of scaling\n",
    "#the data. Once again, the knn model as well as the X and y data and\n",
    "#labels set have already been created for you.\n",
    "\n",
    "# Create the scaling method.\n",
    "#ss = StandardScaler()\n",
    "\n",
    "# Apply the scaling method to the dataset used for modeling.\n",
    "#X_scaled = ss.fit_transform(X)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data.\n",
    "#knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data.\n",
    "#print(knn.score(X_test, y_test))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.9555555555555556\n",
    "#################################################\n",
    "#The increase in accuracy is worth the extra step of scaling the dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Feature engineering**\n",
    "___\n",
    "- What is feature engineering?\n",
    "    - the creation of new features based on existing features\n",
    "    - insight into relationships between features\n",
    "    - extract and expand data\n",
    "    - dataset-dependent\n",
    "- scenarios\n",
    "    - text data\n",
    "    - categorical data\n",
    "    - time stamps\n",
    "    - averages\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Encoding categorical variables**\n",
    "___\n",
    "- encoding binary values\n",
    "    - Pandas\n",
    "        - .apply() plus lambda function\n",
    "        - .get_dummies() for one-hot encoding of variables with two or more labels\n",
    "    - scikit-learn\n",
    "        - LabelEncoder\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Encoding categorical variables - binary\n",
    "\n",
    "#Take a look at the hiking dataset. There are several columns here\n",
    "#that need encoding, one of which is the Accessible column, which\n",
    "#needs to be encoded in order to be modeled. Accessible is a binary\n",
    "#feature, so it has two values - either Y or N - so it needs to be\n",
    "#encoded into 1s and 0s. Use scikit-learn's LabelEncoder method to\n",
    "#do that transformation.\n",
    "\n",
    "# Set up the LabelEncoder object\n",
    "#enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "#hiking[\"Accessible_enc\"] = enc.fit_transform(hiking[\"Accessible\"])\n",
    "\n",
    "# Compare the two columns\n",
    "#print(hiking[[\"Accessible_enc\", \"Accessible\"]].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       Accessible_enc Accessible\n",
    "#    0               1          Y\n",
    "#    1               0          N\n",
    "#    2               0          N\n",
    "#    3               0          N\n",
    "#    4               0          N\n",
    "#################################################\n",
    "#.fit_transform() is a good way to both fit an encoding and\n",
    "#transform the data in a single step."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Encoding categorical variables - one-hot\n",
    "\n",
    "#One of the columns in the volunteer dataset, category_desc, gives\n",
    "#category descriptions for the volunteer opportunities listed.\n",
    "#Because it is a categorical variable with more than two categories,\n",
    "#we need to use one-hot encoding to transform this column numerically.\n",
    "#Use Pandas' get_dummies() function to do so.\n",
    "\n",
    "# Transform the category_desc column\n",
    "#category_enc = pd.get_dummies(volunteer[\"category_desc\"])\n",
    "\n",
    "# Take a look at the encoded columns\n",
    "#print(category_enc.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       Education            ...              Strengthening Communities\n",
    "#    0          0            ...                                      0\n",
    "#    1          0            ...                                      1\n",
    "#    2          0            ...                                      1\n",
    "#    3          0            ...                                      1\n",
    "#    4          0            ...                                      0\n",
    "#\n",
    "#    [5 rows x 6 columns]\n",
    "#\n",
    "#################################################\n",
    "#get_dummies() is a simple and quick way to encode categorical variables."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}