{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Classification with XGBoost**\n",
    "___\n",
    "Introduction:\n",
    "- Supervised learning\n",
    "    - **has labeled data** - we have some understanding of past behavior of problem we are trying to solve or what we are trying to predict\n",
    "    - **Classification** - binary or multi-class\n",
    "        - AUC is metric for binary classification\n",
    "            - Area on receiver operating characteristic curve\n",
    "            - probability that a randomly chosen positive data point will have a higher rank than a randomly chosen negative data point for your data problem\n",
    "            - higher AUC, better model\n",
    "        - for multi-class problems, confusion-matrix and accuracy score is how this happens\n",
    "- features are either numeric or categorical\n",
    "    - numeric features should be scaled (e.g., SVM models)\n",
    "    - categorical features should be encoded (one-hot)\n",
    "- **Ranking** - predicting an ordering on a srt of choices\n",
    "- **Recommending** - recommending an item to a user based on consumption history and profile\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Introduction to XGBoost**\n",
    "___\n",
    "- optimized gradient boosting machine learning library\n",
    "- written in C++\n",
    "- fast\n",
    "- paralellizable"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#XGBoost: Fit/Predict\n",
    "\n",
    "#It's time to create your first XGBoost model! As Sergey showed you\n",
    "#in the video, you can use the scikit-learn .fit() / .predict()\n",
    "#paradigm that you are already familiar to build your XGBoost models,\n",
    "#as the xgboost library has a scikit-learn compatible API!\n",
    "\n",
    "#Here, you'll be working with churn data. This dataset contains imaginary\n",
    "#data from a ride-sharing app with user behaviors over their first month\n",
    "#of app usage in a set of imaginary cities as well as whether they used\n",
    "#the service 5 months after sign-up. It has been pre-loaded for you into\n",
    "#a DataFrame called churn_data - explore it in the Shell!\n",
    "\n",
    "#Your goal is to use the first month's worth of data to predict whether\n",
    "#the app's users will remain users of the service at the 5 month mark.\n",
    "#This is a typical setup for a churn prediction problem. To do this,\n",
    "#you'll split the data into training and test sets, fit a small xgboost\n",
    "#model on the training set, and evaluate its performance on the test set\n",
    "#by computing its accuracy.\n",
    "\n",
    "#pandas and numpy have been imported as pd and np, and train_test_split\n",
    "#has been imported from sklearn.model_selection. Additionally, the arrays\n",
    "#for the features and the target have been created as X and y.\n",
    "\n",
    "# Import xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "#X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the training and test sets\n",
    "#X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "#xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "#xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "#preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "#accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "#print(\"accuracy: %f\" % (accuracy))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    accuracy: 0.743300\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**What is a decision tree?**\n",
    "___\n",
    "- series of binary choices\n",
    "- prediction happens at the \"leaves\" of the tree\n",
    "- **base learner** - individual learning algorithm in an ensemble algorithm\n",
    "- decision trees and CART are constructed iteratively until a stopping criterion is met\n",
    "- individual decision trees tend to overfit\n",
    "    - low bias, high variance\n",
    "    - generalize to new data poorly\n",
    "- **Classification and Regression Trees (CART)**\n",
    "    - each leaf *always* contains a real-valued score which can be later converted into categories\n",
    "___"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Decision trees\n",
    "\n",
    "#Your task in this exercise is to make a simple decision tree using\n",
    "#scikit-learn's DecisionTreeClassifier on the breast cancer dataset\n",
    "#that comes pre-loaded with scikit-learn.\n",
    "\n",
    "#This dataset contains numeric measurements of various dimensions of\n",
    "#individual tumors (such as perimeter and texture) from breast biopsies\n",
    "#and a single outcome value (the tumor is either malignant, or benign).\n",
    "\n",
    "#We've preloaded the dataset of samples (measurements) into X and the\n",
    "#target values per tumor into y. Now, you have to split the complete\n",
    "#dataset into training and testing sets, and then train a\n",
    "#DecisionTreeClassifier. You'll specify a parameter called max_depth.\n",
    "#Many other parameters can be modified within this model, and you can\n",
    "#check all of them out at\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "\n",
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the training and test sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "#dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "#dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "#y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "#accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "#print(\"accuracy:\", accuracy)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#   accuracy: 0.9649122807017544\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**What is Boosting?**\n",
    "___\n",
    "- a concept that can be applied to a set of machine learning models\n",
    "- ensemble meta-algorithm used to convert many weak learners into a strong learner\n",
    "- **weak learner**\n",
    "    - ML algorithm that is slightly better than chance (50/50)\n",
    "- **strong learner**\n",
    "    - any algorithm that can be tuned to achieve good performance\n",
    "- iteratively learning a set of weak models on subsets of the data\n",
    "- weighting each weak prediction according to each weak learner's performance\n",
    "- combine weighted predictions to obtain a single weighted prediction\n",
    "- **Cross-validation** is baked into XGBoost\n",
    "___"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Measuring accuracy\n",
    "\n",
    "#You'll now practice using XGBoost's learning API through its baked\n",
    "#in cross-validation capabilities. As Sergey discussed in the previous\n",
    "#video, XGBoost gets its lauded performance and efficiency gains by\n",
    "#utilizing its own optimized data structure for datasets called a\n",
    "#DMatrix.\n",
    "\n",
    "#In the previous exercise, the input datasets were converted into\n",
    "#DMatrix data on the fly, but when you use the xgboost cv object,\n",
    "#you have to first explicitly convert your data into a DMatrix. So,\n",
    "#that's what you will do here before running cross-validation on\n",
    "#churn_data.\n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "#X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the DMatrix from X and y: churn_dmatrix\n",
    "#churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "#params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "#cv_results = xgb.cv(dtrain=churn_dmatrix, params=params,\n",
    "#                    nfold=3, num_boost_round=5,\n",
    "#                    metrics=\"error\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "#print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "#print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       train-error-mean  train-error-std  test-error-mean  test-error-std\n",
    "#    0           0.28232         0.002366          0.28378        0.001932\n",
    "#    1           0.26951         0.001855          0.27190        0.001932\n",
    "#    2           0.25605         0.003213          0.25798        0.003963\n",
    "#    3           0.25090         0.001845          0.25434        0.003827\n",
    "#    4           0.24654         0.001981          0.24852        0.000934\n",
    "#    0.75148\n",
    "#################################################\n",
    "\n",
    "#Measuring AUC\n",
    "#Now that you've used cross-validation to compute average out-of-sample\n",
    "#accuracy (after converting from an error), it's very easy to compute\n",
    "#any other metric you might be interested in. All you have to do is pass\n",
    "#it (or a list of metrics) in as an argument to the metrics parameter\n",
    "#of xgb.cv().\n",
    "\n",
    "#Your job in this exercise is to compute another common metric used in\n",
    "#binary classification - the area under the curve (\"auc\"). As before,\n",
    "#churn_data is available in your workspace, along with the DMatrix\n",
    "#churn_dmatrix and parameter dictionary params.\n",
    "\n",
    "#Perform cross_validation: cv_results\n",
    "#cv_results = xgb.cv(dtrain=churn_dmatrix, params=params,\n",
    "#                  nfold=3, num_boost_round=5,\n",
    "#                  metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "#print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "#print((cv_results[\"test-auc-mean\"]).iloc[-1])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
    "#    0        0.768893       0.001544       0.767863      0.002820\n",
    "#    1        0.790864       0.006758       0.789157      0.006846\n",
    "#    2        0.815872       0.003900       0.814476      0.005997\n",
    "#    3        0.822959       0.002018       0.821682      0.003912\n",
    "#    4        0.827528       0.000769       0.826191      0.001937\n",
    "#    0.826191\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**When should I use XGBoost?**\n",
    "___\n",
    "- any supervised learning example with:\n",
    "    - large number of training samples (1000+ samples with fewer than 100 features)\n",
    "    - number of features < number of training samples\n",
    "    - a mixture of categorical and numeric features\n",
    "    - just numeric features\n",
    "- do not use XGBoost with:\n",
    "    - image recognition\n",
    "    - computer vision\n",
    "    - natural language processing/understanding\n",
    "    - smaller number of training samples (see above)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Regression Review**\n",
    "___\n",
    "- Common regression metrics\n",
    "    - root mean squared error (RMSE)\n",
    "        - square root of mean of [difference between actual and predicted values, squared]\n",
    "        - treats negative and positive values equally\n",
    "        - tends to punish larger differences between predicted and actual values\n",
    "    - mean absolute error (MAE)\n",
    "        - sums absolute differences\n",
    "- **Algorithms**\n",
    "    - linear regression\n",
    "    - decision trees\n",
    "___"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Objective (loss) functions and base learners**\n",
    "___\n",
    "- Quantifies how far off a prediction is from the actual result\n",
    "- Measures the difference between the estimated true values for some collection of data\n",
    "- **Goal**: find the model that yields the minimum value of the loss function\n",
    "- in xgboost:\n",
    "    - reg:linear - use for regression problems\n",
    "    - reg:logistic - use for classification problems when you want decision, not probability\n",
    "    - binary:logistic - use when you want probability rather than just decision\n",
    "- base learners and why we need them\n",
    "        - we want base learners that when combined create a final prediction that is **non-linear**\n",
    "        - each base learner should be good at distinguishing or predicting different parts of the dataset\n",
    "- two kinds of base learners: tree and linear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Decision trees as base learners\n",
    "\n",
    "#It's now time to build an XGBoost model to predict house prices -\n",
    "#not in Boston, Massachusetts, as you saw in the video, but in Ames,\n",
    "#Iowa! This dataset of housing prices has been pre-loaded into a\n",
    "#DataFrame called df. If you explore it in the Shell, you'll see that\n",
    "#there are a variety of features about the house and its location in\n",
    "#the city.\n",
    "\n",
    "#In this exercise, your goal is to use trees as base learners. By default,\n",
    "#XGBoost uses trees as base learners, so you don't have to specify that you\n",
    "#want to use trees here with booster=\"gbtree\".\n",
    "\n",
    "#xgboost has been imported as xgb and the arrays for the features and\n",
    "#the target are available in X and y, respectively.\n",
    "\n",
    "# Create the training and test sets\n",
    "#X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBRegressor: xg_reg\n",
    "#xg_reg = xgb.XGBRegressor(objective=\"reg:linear\", n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the regressor to the training set\n",
    "#xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "#preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "#rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "#print(\"RMSE: %f\" % (rmse))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [18:40:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
    "#    RMSE: 78847.401758\n",
    "#################################################\n",
    "\n",
    "#Linear base learners\n",
    "\n",
    "#Now that you've used trees as base models in XGBoost, let's use the\n",
    "#other kind of base model that can be used with XGBoost - a linear\n",
    "#learner. This model, although not as commonly used in XGBoost, allows\n",
    "#you to create a regularized linear regression using XGBoost's powerful\n",
    "#learning API. However, because it's uncommon, you have to use XGBoost's\n",
    "#own non-scikit-learn compatible functions to build the model, such as\n",
    "#xgb.train().\n",
    "\n",
    "#In order to do this you must create the parameter dictionary that\n",
    "#describes the kind of booster you want to use (similarly to how you\n",
    "#created the dictionary in Chapter 1 when you used xgb.cv()). The\n",
    "#key-value pair that defines the booster type (base model) you need is\n",
    "#\"booster\":\"gblinear\".\n",
    "\n",
    "#Once you've created the model, you can use the .train() and .predict()\n",
    "#methods of the model just like you've done in the past.\n",
    "\n",
    "#Here, the data has already been split into training and testing sets,\n",
    "#so you can dive right into creating the DMatrix objects required by the\n",
    "#XGBoost learning API.\n",
    "\n",
    "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
    "#DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "#DM_test =  xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "#params = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "#xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "#preds = xg_reg.predict(DM_test)\n",
    "\n",
    "# Compute and print the RMSE\n",
    "#rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "#print(\"RMSE: %f\" % (rmse))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [18:52:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
    "#    RMSE: 40738.238504\n",
    "#################################################\n",
    "\n",
    "#Evaluating model quality\n",
    "#It's now time to begin evaluating model quality.\n",
    "\n",
    "#Here, you will compare the RMSE and MAE of a cross-validated XGBoost\n",
    "#model on the Ames housing data. As in previous exercises, all necessary\n",
    "#modules have been pre-loaded and the data is available in the DataFrame\n",
    "#df.\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "#housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "#params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "#cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"mae\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "#print(cv_results)\n",
    "\n",
    "# Extract and print final round boosting round metric\n",
    "#print((cv_results[\"test-mae-mean\"]).tail(1))\n",
    "\n",
    "#results printed below for \"rmse\" first and \"mae\" second\n",
    "#################################################\n",
    "#train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
    "#    0    141767.531250      429.454591   142980.429688    1193.794436\n",
    "#    1    102832.544922      322.474657   104891.394532    1223.158855\n",
    "#    2     75872.619140      266.472468    79478.935547    1601.344218\n",
    "#    3     57245.650390      273.624608    62411.922851    2220.149653\n",
    "#    4     44401.297851      316.422372    51348.279297    2963.377719\n",
    "#    4    51348.279297\n",
    "#   Name: test-rmse-mean, dtype: float64\n",
    "\n",
    "#train-mae-mean  train-mae-std  test-mae-mean  test-mae-std\n",
    "#    0   127343.476562     668.342129  127633.980469   2404.003469\n",
    "#    1    89770.052735     456.962096   90122.501953   2107.915156\n",
    "#    2    63580.791992     263.403452   64278.561524   1887.563452\n",
    "#    3    45633.153321     151.884551   46819.167969   1459.819091\n",
    "#    4    33587.092774      86.999100   35670.644531   1140.607997\n",
    "#    4    35670.644531\n",
    "#    Name: test-mae-mean, dtype: float64\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Regularization and base learners in XGBoost**\n",
    "___\n",
    "- Regularization is a control on model complexity\n",
    "- Want models that are both accurate and as simple as possible\n",
    "- Regularization parameters in XGBoost\n",
    "    - *gamma* - minimum loss reduction allowed for a split to occur (higher value means fewer splits)\n",
    "    - *alpha* - l1 regularization (many weights will go to zero) of leaf weights (higher values mean more regularization)\n",
    "    - *lambda* - l2 regularization (smooths leaf weights)\n",
    "- Linear base learner\n",
    "    - sum of linear terms\n",
    "    - boosted model is weighted sum of linear models (linear)\n",
    "    - rarely used (you can get same performance from regularized linear model\n",
    "- Tree base learner\n",
    "    - decision tree\n",
    "    - boosted model is weighted sum of decision trees (nonlinear)\n",
    "    - almost always used in XGBoost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Using regularization in XGBoost\n",
    "#Having seen an example of l1 regularization in the video, you'll now\n",
    "#vary the l2 regularization penalty - also known as \"lambda\" - and see\n",
    "#its effect on overall model performance on the Ames housing dataset.\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "#housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "#reg_params = [1, 10, 100]\n",
    "\n",
    "# Create the initial parameter dictionary for varying l2 strength: params\n",
    "#params = {\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create an empty list for storing rmses as a function of l2 complexity\n",
    "#rmses_l2 = []\n",
    "\n",
    "# Iterate over reg_params\n",
    "#for reg in reg_params:\n",
    "\n",
    "    # Update l2 strength\n",
    "#    params[\"lambda\"] = reg\n",
    "\n",
    "    # Pass this updated param dictionary into cv\n",
    "#    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "    # Append best rmse (final round) to rmses_l2\n",
    "#    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
    "\n",
    "# Look at best rmse per l2 param\n",
    "#print(\"Best rmse as a function of l2:\")\n",
    "#print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\",\"rmse\"]))\n",
    "\n",
    "#################################################\n",
    "#Best rmse as a function of l2:\n",
    "#        l2          rmse\n",
    "#    0    1  52275.359375\n",
    "#    1   10  57746.064453\n",
    "#    2  100  76624.625000\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visualizing individual XGBoost trees\n",
    "#Now that you've used XGBoost to both build and evaluate regression as\n",
    "#well as classification models, you should get a handle on how to\n",
    "#visually explore your models. Here, you will visualize individual trees\n",
    "#from the fully boosted model that XGBoost creates using the entire\n",
    "#housing dataset.\n",
    "\n",
    "#XGBoost has a plot_tree() function that makes this type of\n",
    "#visualization easy. Once you train a model using the XGBoost learning\n",
    "#API, you can pass it to the plot_tree() function along with the number\n",
    "#of trees you want to plot using the num_trees argument.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/11.1.svg](_images/11.1.svg)\n",
    "![_images/11.2.svg](_images/11.2.svg)\n",
    "![_images/11.3.svg](_images/11.3.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}